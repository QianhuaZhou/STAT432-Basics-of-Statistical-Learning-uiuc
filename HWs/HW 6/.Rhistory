for (i in 1:nrow(Xtrain)) {
weights[i] <- gaussian_kernel(Xtest, Xtrain[i, ], h)
}
return(weights)
}
# Define the bandwidth
h <- 0.07
# Generate the design matrix with an intercept term
Xtrain_augmented <- cbind(1, Xtrain)  # Adding a column of 1s for the intercept
# Calculate the kernel weights
weights <- calculate_kernel_weights(Xtrain, Xtest, h)
# Create the diagonal weight matrix W
W <- diag(weights)
# Estimate beta_x0 using the normal equation
beta_x0 <- solve(t(Xtrain_augmented) %*% W %*% Xtrain_augmented) %*% t(Xtrain_augmented) %*% W %*% Ytrain
# Report the estimated beta_x0
cat("Estimated beta_x0: ", beta_x0, "\n")
# Predict the response at the test point
Xtest_augmented <- c(1, Xtest)  # Add an intercept term for the test point
predicted_value <- sum(Xtest_augmented * beta_x0)
# Calculate the true expectation for the test point
true_value <- exp(Xtest %*% beta)
# Compare the predicted value with the true expectation
cat("Predicted value: ", predicted_value, "\n")
cat("True value: ", true_value, "\n")
set.seed(432)
# Generate 100 testing points
testn <- 100
Xtest <- matrix(runif(testn * p), ncol = p)
# True expectation for the test points
true_values <- exp(Xtest %*% beta)
# Local Polynomial Regression Function (from previous parts)
predict_local_polynomial <- function(Xtrain, Ytrain, Xtest, h) {
predictions <- numeric(nrow(Xtest))
for (i in 1:nrow(Xtest)) {
beta_est <- estimate_beta(Xtrain, Ytrain, Xtest[i, ], h)
predictions[i] <- c(1, Xtest[i, ]) %*% beta_est
}
return(predictions)
}
# Set bandwidth h
h <- 0.07
# Predict using local polynomial regression for the 100 test points
local_predictions <- predict_local_polynomial(Xtrain, Ytrain, Xtest, h)
# Global Linear Regression Model
# We need to adjust the Xtrain for the global model
Xtrain_df <- data.frame(Xtrain)
# Fit global linear regression model
global_model <- lm(Ytrain ~ ., data = Xtrain_df)
# Make predictions for the 100 test points
Xtest_df <- data.frame(Xtest)  # Match the structure of Xtrain
global_predictions <- predict(global_model, newdata = Xtest_df)
# Scatter plot for Local Polynomial Regression
plot(true_values, local_predictions, xlab = "True Expectation", ylab = "Predicted Values",
main = "True vs Predicted Values (Local Polynomial Regression)", col = "blue", pch = 19)
abline(0, 1, col = "red")  # Line of perfect prediction
# Scatter plot for Global Linear Regression
plot(true_values, global_predictions, xlab = "True Expectation", ylab = "Predicted Values",
main = "True vs Predicted Values (Global Linear Regression)", col = "blue", pch = 19)
abline(0, 1, col = "red")  # Line of perfect prediction
# Compare the performance of local and global models
local_mse <- mean((local_predictions - true_values)^2)
global_mse <- mean((global_predictions - true_values)^2)
cat("Local Polynomial Regression MSE:", local_mse, "\n")
cat("Global Linear Regression MSE:", global_mse, "\n")
if (local_mse < global_mse) {
cat("The local polynomial regression model performs better.\n")
} else {
cat("The global linear regression model performs better.\n")
}
set.seed(432)  # Set a different seed for generating test points
# Generate 100 testing points
testn <- 100
Xtest <- matrix(runif(testn * p), ncol = p)  # 100 random test points
# Function to predict using local polynomial regression
predict_local_polynomial <- function(Xtrain, Ytrain, Xtest, h) {
predictions <- numeric(nrow(Xtest))
for (i in 1:nrow(Xtest)) {
# Calculate kernel weights for the i-th test point
weights <- calculate_kernel_weights(Xtrain, Xtest[i, ], h)
W <- diag(weights)
Xtrain_augmented <- cbind(1, Xtrain)  # Augment training data with intercept
beta_x0 <- solve(t(Xtrain_augmented) %*% W %*% Xtrain_augmented) %*% t(Xtrain_augmented) %*% W %*% Ytrain
Xtest_augmented <- c(1, Xtest[i, ])  # Add intercept for test point
predictions[i] <- sum(Xtest_augmented * beta_x0)  # Predict using beta
}
return(predictions)
}
# Predict using the local polynomial regression model for 100 test points
h <- 0.07
local_predictions <- predict_local_polynomial(Xtrain, Ytrain, Xtest, h)
# Calculate the true expectations for the 100 test points
true_values <- exp(Xtest %*% beta)
# Fit a global linear regression model
Xtrain_df <- data.frame(Xtrain)
global_model <- lm(Ytrain ~ ., data = Xtrain_df)
# Predict the responses for the 100 test points using the global model
Xtest_df <- data.frame(Xtest)
global_predictions <- predict(global_model, newdata = Xtest_df)
# Scatter plot of true vs predicted values (Local Polynomial Regression)
plot(true_values, local_predictions, xlab = "True Expectations", ylab = "Predicted Values",
main = "True vs Predicted Values (Local Polynomial Regression)", col = "blue", pch = 19)
abline(0, 1, col = "red")  # Line of perfect prediction
# Scatter plot of true vs predicted values (Global Linear Regression)
plot(true_values, global_predictions, xlab = "True Expectations", ylab = "Predicted Values",
main = "True vs Predicted Values (Global Linear Regression)", col = "blue", pch = 19)
abline(0, 1, col = "red")  # Line of perfect prediction
# Compare model performance by calculating the Mean Squared Error (MSE)
local_mse <- mean((local_predictions - true_values)^2)
global_mse <- mean((global_predictions - true_values)^2)
cat("Local Polynomial Regression MSE:", local_mse, "\n")
cat("Global Linear Regression MSE:", global_mse, "\n")
# Determine which model performs better
if (local_mse < global_mse) {
cat("The local polynomial regression model performs better.\n")
} else {
cat("The global linear regression model performs better.\n")
}
# Extend the simulate_kernel_regression function to return bias_squared and variance
find_optimal_bandwidth_with_decomposition <- function(h_values, trainn, testn, p, beta, B) {
mse_values <- numeric(length(h_values))
bias_squared_values <- numeric(length(h_values))
variance_values <- numeric(length(h_values))
for (i in 1:length(h_values)) {
h <- h_values[i]
res <- simulate_kernel_regression(trainn = trainn, testn = testn, p = p, beta = beta, h = h, B = B)
mse_values[i] <- res$mse
bias_squared_values[i] <- res$bias_squared
variance_values[i] <- res$variance
}
optimal_h <- h_values[which.min(mse_values)]
return(list(optimal_h = optimal_h, mse_values = mse_values,
bias_squared_values = bias_squared_values, variance_values = variance_values))
}
# Test a range of bandwidths
h_values <- seq(0.01, 0.2, by = 0.01)
optimal_bandwidth_decomposition <- find_optimal_bandwidth_with_decomposition(h_values, trainn = 200, testn = 1, p = 2, beta = beta, B = 5000)
# Chunk 1
.solution {
# Define Gaussian kernel function
gaussian_kernel <- function(x0, xi, h) {
return((1 / (h * sqrt(2 * pi))) * exp(-((x0 - xi)^2) / (2 * h^2)))
}
# Define bandwidth
h <- 0.07
# Calculate kernel weights for each training data point against the test point
calculate_kernel_weights <- function(Xtrain, Xtest, h) {
weights <- numeric(nrow(Xtrain))
for (i in 1:nrow(Xtrain)) {
weights[i] <- gaussian_kernel(Xtest, Xtrain[i, ], h)
}
return(weights)
}
# Calculate the kernel weights
weights <- calculate_kernel_weights(Xtrain, Xtest, h)
# Report the 25th, 50th, and 75th percentiles of the weights
quantiles <- quantile(weights, probs = c(0.25, 0.5, 0.75))
print(quantiles)
library(gurobi)
library(MASS)  # For computing Mahalanobis distance
library(stats)  # For fitting logistic regression model to compute propensity scores
mip_matching <- function(treated_units, control_units, treated_covariates, control_covariates, distance_method = "mahalanobis", m = 1, sigma = NULL, mom_num = 2, mom_tol = NULL) {
#check if m is valid
m_max = floor(control_units / treated_units)
if (m > m_max) {
return(cat("m is invalid. Maximum value of m is", m_max))
}
# Calculate distance matrix
if (distance_method == "mahalanobis") {
# 1. Using Mahalanobis Distance
combined_covariates <- rbind(treated_covariates, control_covariates)
cov_matrix <- cov(combined_covariates)  # Compute covariance matrix
dist <- matrix(0, nrow = treated_units, ncol = control_units)
for (i in 1:treated_units) {
for (j in 1:control_units) {
dist[i, j] <- mahalanobis(treated_covariates[i, ], control_covariates[j, ], cov_matrix)
}
}
} else if (distance_method == "propensity") {
# 2. Using Propensity Score Distance
# Create a data frame for fitting the logistic regression model
combined_data <- data.frame(rbind(treated_covariates, control_covariates))
combined_data$treatment <- c(rep(1, treated_units), rep(0, control_units))
# Fit a logistic regression model to calculate propensity scores
propensity_model <- glm(treatment ~ ., data = combined_data, family = binomial())
propensity_scores <- predict(propensity_model, type = "response")
# Separate propensity scores for treated and control units
treated_scores <- propensity_scores[1:treated_units]
control_scores <- propensity_scores[(treated_units + 1):(treated_units + control_units)]
# Calculate the absolute difference in propensity scores as the distance
dist <- outer(treated_scores, control_scores, FUN = function(x, y) abs(x - y))
} else {
stop("Invalid distance method. Choose either 'mahalanobis' or 'propensity'.")
}
print(dist)
# Model setup
model <- list()
# Objective function: Minimize total distance + penalty(added)
penalty_matrix <- matrix(0, nrow = treated_units, ncol = control_units)
if (!is.null(omega)) {
for (i in 1:treated_units) {
for (j in 1:control_units) {
penalty_matrix[i, j] <- sum(omega * abs(treated_covariates[i, ] - control_covariates[j, ]))
}
}
}
# Objective function: Minimize total distance
#model$obj <- as.vector(t(dist))
model$obj <- as.vector(t(dist)) + as.vector(t(penalty_matrix))#added
# Constraint 1.2: Each treated unit must be matched to m control units
model$A <- matrix(0, nrow = treated_units, ncol = treated_units * control_units)
for (i in 1:treated_units) {
model$A[i, seq((i - 1)*control_units+1, i*control_units)] <- 1
}
model$sense <- rep("=", treated_units)
model$rhs <- rep(m, treated_units)
# Constraint 1.3: Each control unit can be used at most once
control_constraint = matrix(0, nrow = control_units, ncol = treated_units * control_units)
for (j in 1:control_units) {
control_constraint[j, seq(j, treated_units * control_units, by = control_units)] = 1
}
model$A = rbind(model$A, control_constraint)
model$sense <- c(model$sense, rep("<=", control_units))
model$rhs <- c(model$rhs, rep(1, control_units))
# Constraint 1.5: mean difference of each covariate should be <= sigma[j]
if (!is.null(sigma)) {
num_covariates <- length(sigma)
for (j in 1:num_covariates) {
mean_diff = matrix(0, nrow = 2, ncol = treated_units * control_units)
for (i in 1:treated_units) {
for (k in 1:control_units) {
mean_diff[ ,(i-1)*control_units + k] = treated_covariates[i, j] - control_covariates[k, j]
}
}
model$A <- rbind(model$A, mean_diff)
model$sense <- c(model$sense, "<=", ">=")
model$rhs <- c(model$rhs, sigma[j] * (m*treated_units), -sigma[j] * (m*treated_units))
}
}
# Constraint 1.6: higher moment differences of each covariate <= mom_tol[j]
if (!is.null(mom_tol)) {
num_covariates <- length(sigma)
for (j in 1:num_covariates) {
mom_diff = matrix(0, nrow = 2, ncol = treated_units * control_units)
for (i in 1:treated_units) {
for (k in 1:control_units) {
mom_diff[ ,(i-1)*control_units + k] = treated_covariates[i, j]^mom_num - control_covariates[k, j]^mom_num
}
}
model$A <- rbind(model$A, mom_diff)
model$sense <- c(model$sense, "<=", ">=")
model$rhs <- c(model$rhs, mom_tol[j] * (m*treated_units), -mom_tol[j] * (m*treated_units))
}
}
# NEW CONSTRAINT: Absolute difference in covariates should be <= sigma(added)
if (!is.null(sigma)) {
num_covariates <- length(sigma)
for (j in 1:num_covariates) {
for (i in 1:treated_units) {
for (k in 1:control_units) {
abs_diff_pos <- rep(0, treated_units * control_units)
abs_diff_neg <- rep(0, treated_units * control_units)
# goal: |X_t_i - X_c_i| <= sigma_i
# Positive constraint: X_t - X_c <= sigma_j
abs_diff_pos[(i - 1) * control_units + k] <- treated_covariates[i, j] - control_covariates[k, j]
# Negative constraint: X_c - X_t <= sigma_j
abs_diff_neg[(i - 1) * control_units + k] <- control_covariates[k, j] - treated_covariates[i, j]
# Add to model
model$A <- rbind(model$A, abs_diff_pos, abs_diff_neg)
model$sense <- c(model$sense, "<=", "<=")
model$rhs <- c(model$rhs, sigma[j], sigma[j])
}
}
}
}
# Variable types 1.4: Binary (0 or 1)
model$vtype <- rep("B", treated_units * control_units)
# Solve the MIP using Gurobi
params <- list(OutputFlag = 0)  # Suppress Gurobi output
result <- gurobi(model, params = params)
#print(result)
# Check if a feasible solution was found
if (!is.null(result$x)) {
# Reshape the solution into a matching matrix
matching_matrix <- matrix(result$x, nrow = treated_units, ncol = control_units, byrow = TRUE)
return(list(success = TRUE, matching_matrix = matching_matrix))
} else {
return(list(success = FALSE, message = "No feasible solution found"))
}
}
param = function(df) {
t_num = sum(data["treat"])
c_num = nrow(data) - t_num
t_matrix = as.matrix(data[data["treat"]==1,])
c_matrix = as.matrix(data[data["treat"]==0,])
return(list(treated_units = t_num, control_units = c_num,
treated_covariates = t_matrix, control_covariates = c_matrix))
}
match_mean = function(data, cov_num, matrix) {
controls = which(apply(matrix, 2, function(col) any(col == 1)))
mean_matrix = matrix(0, nrow = 2, ncol = length(cov_num))
for (i in 1:length(cov_num)) {
cur_cov = cov_num[i]
cur_treat = mean(data[data["treat"]==1,][,cur_cov])
cur_control = mean(data[controls,][,cur_cov])
mean_matrix[ ,i] = c(cur_treat, cur_control)
}
return(mean_matrix)
}
# Example usage
set.seed(123)
treated_units <- 5
control_units <- 10
treated_covariates <- matrix(runif(treated_units * 3, min = 20, max = 60), nrow = treated_units, ncol = 3)
control_covariates <- matrix(runif(control_units * 3, min = 20, max = 60), nrow = control_units, ncol = 3)
# Penalty weights for covariates
omega <- c(1, 1, 1)
# Maximum allowable differences for covariates
sigma <- c(5, 5, 5)
# Perform matching with Mahalanobis distance, penalty, and covariate constraints
result_mahalanobis <- mip_matching(treated_units, control_units, treated_covariates, control_covariates, distance_method = "mahalanobis", m = 1, sigma = sigma, omega = omega)
# Example usage
set.seed(123)
treated_units <- 5
control_units <- 10
# Generate random covariates (3 covariates per unit)
treated_covariates <- matrix(runif(treated_units * 3, min = 20, max = 60), nrow = treated_units, ncol = 3)
control_covariates <- matrix(runif(control_units * 3, min = 20, max = 60), nrow = control_units, ncol = 3)
# Penalty weights for covariates
omega <- c(1, 1, 1)
# Maximum allowable differences for covariates (σ values)
sigma <- c(5, 5, 5)
# Perform matching with Mahalanobis distance, penalty, and covariate constraints
result_mahalanobis <- mip_matching(treated_units, control_units, treated_covariates, control_covariates, distance_method = "mahalanobis", m = 1, sigma = sigma, omega = omega)
library(gurobi)
library(MASS)  # For computing Mahalanobis distance
library(stats)  # For fitting logistic regression model to compute propensity scores
mip_matching <- function(treated_units, control_units, treated_covariates, control_covariates, distance_method = "mahalanobis", m = 1, sigma = NULL, mom_num = 2, mom_tol = NULL, omega = NULL)) {
library(gurobi)
library(MASS)  # For computing Mahalanobis distance
library(stats)  # For fitting logistic regression model to compute propensity scores
mip_matching <- function(treated_units, control_units, treated_covariates, control_covariates, distance_method = "mahalanobis", m = 1, sigma = NULL, mom_num = 2, mom_tol = NULL, omega = NULL){
#check if m is valid
m_max = floor(control_units / treated_units)
if (m > m_max) {
return(cat("m is invalid. Maximum value of m is", m_max))
}
# Calculate distance matrix
if (distance_method == "mahalanobis") {
# 1. Using Mahalanobis Distance
combined_covariates <- rbind(treated_covariates, control_covariates)
cov_matrix <- cov(combined_covariates)  # Compute covariance matrix
dist <- matrix(0, nrow = treated_units, ncol = control_units)
for (i in 1:treated_units) {
for (j in 1:control_units) {
dist[i, j] <- mahalanobis(treated_covariates[i, ], control_covariates[j, ], cov_matrix)
}
}
} else if (distance_method == "propensity") {
# 2. Using Propensity Score Distance
# Create a data frame for fitting the logistic regression model
combined_data <- data.frame(rbind(treated_covariates, control_covariates))
combined_data$treatment <- c(rep(1, treated_units), rep(0, control_units))
# Fit a logistic regression model to calculate propensity scores
propensity_model <- glm(treatment ~ ., data = combined_data, family = binomial())
propensity_scores <- predict(propensity_model, type = "response")
# Separate propensity scores for treated and control units
treated_scores <- propensity_scores[1:treated_units]
control_scores <- propensity_scores[(treated_units + 1):(treated_units + control_units)]
# Calculate the absolute difference in propensity scores as the distance
dist <- outer(treated_scores, control_scores, FUN = function(x, y) abs(x - y))
} else {
stop("Invalid distance method. Choose either 'mahalanobis' or 'propensity'.")
}
print(dist)
# Model setup
model <- list()
# Objective function: Minimize total distance + penalty(added)
penalty_matrix <- matrix(0, nrow = treated_units, ncol = control_units)
if (!is.null(omega)) {
for (i in 1:treated_units) {
for (j in 1:control_units) {
penalty_matrix[i, j] <- sum(omega * abs(treated_covariates[i, ] - control_covariates[j, ]))
}
}
}
# Objective function: Minimize total distance
#model$obj <- as.vector(t(dist))
model$obj <- as.vector(t(dist)) + as.vector(t(penalty_matrix))#added
# Constraint 1.2: Each treated unit must be matched to m control units
model$A <- matrix(0, nrow = treated_units, ncol = treated_units * control_units)
for (i in 1:treated_units) {
model$A[i, seq((i - 1)*control_units+1, i*control_units)] <- 1
}
model$sense <- rep("=", treated_units)
model$rhs <- rep(m, treated_units)
# Constraint 1.3: Each control unit can be used at most once
control_constraint = matrix(0, nrow = control_units, ncol = treated_units * control_units)
for (j in 1:control_units) {
control_constraint[j, seq(j, treated_units * control_units, by = control_units)] = 1
}
model$A = rbind(model$A, control_constraint)
model$sense <- c(model$sense, rep("<=", control_units))
model$rhs <- c(model$rhs, rep(1, control_units))
# Constraint 1.5: mean difference of each covariate should be <= sigma[j]
if (!is.null(sigma)) {
num_covariates <- length(sigma)
for (j in 1:num_covariates) {
mean_diff = matrix(0, nrow = 2, ncol = treated_units * control_units)
for (i in 1:treated_units) {
for (k in 1:control_units) {
mean_diff[ ,(i-1)*control_units + k] = treated_covariates[i, j] - control_covariates[k, j]
}
}
model$A <- rbind(model$A, mean_diff)
model$sense <- c(model$sense, "<=", ">=")
model$rhs <- c(model$rhs, sigma[j] * (m*treated_units), -sigma[j] * (m*treated_units))
}
}
# Constraint 1.6: higher moment differences of each covariate <= mom_tol[j]
if (!is.null(mom_tol)) {
num_covariates <- length(sigma)
for (j in 1:num_covariates) {
mom_diff = matrix(0, nrow = 2, ncol = treated_units * control_units)
for (i in 1:treated_units) {
for (k in 1:control_units) {
mom_diff[ ,(i-1)*control_units + k] = treated_covariates[i, j]^mom_num - control_covariates[k, j]^mom_num
}
}
model$A <- rbind(model$A, mom_diff)
model$sense <- c(model$sense, "<=", ">=")
model$rhs <- c(model$rhs, mom_tol[j] * (m*treated_units), -mom_tol[j] * (m*treated_units))
}
}
# NEW CONSTRAINT: Absolute difference in covariates should be <= sigma(added)
if (!is.null(sigma)) {
num_covariates <- length(sigma)
for (j in 1:num_covariates) {
for (i in 1:treated_units) {
for (k in 1:control_units) {
abs_diff_pos <- rep(0, treated_units * control_units)
abs_diff_neg <- rep(0, treated_units * control_units)
# goal: |X_t_i - X_c_i| <= sigma_i
# Positive constraint: X_t - X_c <= sigma_j
abs_diff_pos[(i - 1) * control_units + k] <- treated_covariates[i, j] - control_covariates[k, j]
# Negative constraint: X_c - X_t <= sigma_j
abs_diff_neg[(i - 1) * control_units + k] <- control_covariates[k, j] - treated_covariates[i, j]
# Add to model
model$A <- rbind(model$A, abs_diff_pos, abs_diff_neg)
model$sense <- c(model$sense, "<=", "<=")
model$rhs <- c(model$rhs, sigma[j], sigma[j])
}
}
}
}
# Variable types 1.4: Binary (0 or 1)
model$vtype <- rep("B", treated_units * control_units)
# Solve the MIP using Gurobi
params <- list(OutputFlag = 0)  # Suppress Gurobi output
result <- gurobi(model, params = params)
#print(result)
# Check if a feasible solution was found
if (!is.null(result$x)) {
# Reshape the solution into a matching matrix
matching_matrix <- matrix(result$x, nrow = treated_units, ncol = control_units, byrow = TRUE)
return(list(success = TRUE, matching_matrix = matching_matrix))
} else {
return(list(success = FALSE, message = "No feasible solution found"))
}
}
param = function(df) {
t_num = sum(data["treat"])
c_num = nrow(data) - t_num
t_matrix = as.matrix(data[data["treat"]==1,])
c_matrix = as.matrix(data[data["treat"]==0,])
return(list(treated_units = t_num, control_units = c_num,
treated_covariates = t_matrix, control_covariates = c_matrix))
}
match_mean = function(data, cov_num, matrix) {
controls = which(apply(matrix, 2, function(col) any(col == 1)))
mean_matrix = matrix(0, nrow = 2, ncol = length(cov_num))
for (i in 1:length(cov_num)) {
cur_cov = cov_num[i]
cur_treat = mean(data[data["treat"]==1,][,cur_cov])
cur_control = mean(data[controls,][,cur_cov])
mean_matrix[ ,i] = c(cur_treat, cur_control)
}
return(mean_matrix)
}
# Example usage
set.seed(123)
treated_units <- 5
control_units <- 10
# Generate random covariates (3 covariates per unit)
treated_covariates <- matrix(runif(treated_units * 3, min = 20, max = 60), nrow = treated_units, ncol = 3)
control_covariates <- matrix(runif(control_units * 3, min = 20, max = 60), nrow = control_units, ncol = 3)
# Penalty weights for covariates
omega <- c(1, 1, 1)
# Maximum allowable differences for covariates (σ values)
sigma <- c(5, 5, 5)
# Perform matching with Mahalanobis distance, penalty, and covariate constraints
result_mahalanobis <- mip_matching(treated_units, control_units, treated_covariates, control_covariates, distance_method = "mahalanobis", m = 1, sigma = sigma, omega = omega)
# Display the result
if (result_mahalanobis$success) {
print("Matching result (1 means treated unit is matched to the control):")
print(result_mahalanobis$matching_matrix)
# Calculate penalty of the matching
penalty <- calculate_penalty(result_mahalanobis$matching_matrix, treated_covariates, control_covariates, omega)
cat("Total penalty:", penalty, "\n")
} else {
print(result_mahalanobis$message)
}
