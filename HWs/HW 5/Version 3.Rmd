---
title: "Stat 432 Homework 5"
date: "Assigned: Sep 23, 2024; <span style='color:red'>Due: 11:59 PM CT, Oct 3, 2024</span>"
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
editor_options: 
  chunk_output_type: console
---

<style>
  body {
  text-align: justify}
</style>

```{css, echo=FALSE}
  .solution {
  background-color: #CCDDFF; /* Replace with your desired color */
  }
  
  blockquote {
  background-color: #CCDDFF; /* Replace with your desired color */
  font-family: "Times New Roman", serif; /* Change to your desired font family */
  /* font-size: 16px; /* Change to your desired font size */
  font-weight: bold; /* Makes the text bold */
  }
```



## Question 1: Regression KNN and Bias-Variance Trade-Off (20 pts)

In our previous homework, we only used the prediction errors to evaluate the performance of a model. Now we have learned how to break down the bias-variance trade-off theoretically, and showed some simulation ideas to validate that in class. Let's perform a thorough investigation. For this question, we will use a simulated regression model to estimate the bias and variance, and then validate our formula. Our simulation is based on this following model:

\[ 
Y = \exp(\beta^T x) + \epsilon
\]

where \(\beta = c(0.5, -0.5, 0) \), $X$ is generated uniformly from $[0, 1]^3$, and $\epsilon$ follows i.i.d. standard Gaussian. We will generate some training data and our goal is to predict a testing point at \(x_0 = c(1, -0.75, -0.7)\). 

  a. [1 pt] What is the true mean of \(Y\) at this testing point \(x_0\)? Calculate it in R. 

```{r}
beta <- c(0.5, -0.5, 0)
x0 <- c(1, -0.75, -0.7)
f_x0 <- exp(sum(beta * x0))
f_x0
```

  b. [5 pts] For this question, you need to __write your own code__ for implementing KNN, rather than using any built-in functions in R. Generate 100 training data points and calculate the KNN prediction of $x_0$ with \(k = 21\). Use the Euclidean distance as the distance metric. What is your prediction? Validate your result with the `knn.reg` function from the `FNN` package.
```{r}
#install.packages("FNN")
library(FNN)
set.seed(123) 
n <- 100
X_train <- matrix(runif(n * 3, 0, 1), ncol = 3)
epsilon <- rnorm(n)
Y_train <- exp(X_train %*% beta) + epsilon

knn_custom <- function(x0, X_train, Y_train, k) {
  distances <- apply(X_train, 1, function(row) sqrt(sum((row - x0)^2)))
  nearest_indices <- order(distances)[1:k]
  mean(Y_train[nearest_indices])
}

k <- 21
knn_pred <- knn_custom(x0, X_train, Y_train, k)
knn_pred

#validate with knn.reg
knn_pred_validation <- knn.reg(X_train, test = matrix(x0, nrow = 1), y = Y_train, k = k)$pred
knn_pred_validation
```
  c. [5 pts] Now we will estimate the bias of the KNN model for predicting $x_0$. Use the KNN code you developed in the previous question. To estimate the bias, you need to perform a simulation that repeats 1000 times. Keep in mind that the bias of a model is defined as $E[\widehat f(x_0)] - f(x_0)$. Use the same sample size $n = 100$ and same $k = 21$, design your own simulation study to estimate this.
  
```{r}
set.seed(123)
num_simulations <- 1000
knn_preds <- numeric(num_simulations)

for (i in 1:num_simulations) {
  X_train <- matrix(runif(n * 3, 0, 1), ncol = 3)
  epsilon <- rnorm(n)
  Y_train <- exp(X_train %*% beta) + epsilon
  knn_preds[i] <- knn_custom(x0, X_train, Y_train, k)
}

estimated_bias <- mean(knn_preds) - f_x0
cat("The Bias:", estimated_bias, "\n")
```
  d. [2 pt] Based on your previous simulation, without generating new simulation results, can you estimate the variance of this model? The variance of a model is defined as $E[(\widehat f(x_0) - E[\widehat f(x_0)])^2]$. Calculate and report the value. 
```{r}
estimated_variance <- var(knn_preds)
cat("The Variance:", estimated_variance, "\n")
```
The estimated variance of this model is 0.05027535.

  e. [2 pts] Recall that our prediction error (using this model of predicted probability with knn) can be decomposed into the irreducible error, bias, and variance. Without performing additional simulations, can you calculate each of them based on our model and the previous simulation results? Hence what is your calculated prediction error? 
  
From previous question, the estimated bias is -1.172217 and the estimated variance is 0.05027535. The irreducible error is the variance of the noise term, which is 1(since $\epsilon$ ~ N(0, 1)). The prediction error is the sum of the squared bias, variance, and irreducible error. 

```{r}
prediction_error <- 1 + (estimated_bias^2) + estimated_variance
cat("The Calculated Prediction Error:", prediction_error, "\n")
```


  f. [5 pts] The last step is to validate this result. To do this, you should generate a testing data $Y_0$ using $x_0$ in each of your simulation run, and calculate the prediction error. Compare this result with your theoritical calculation.
```{r}
prediction_errors <- numeric(num_simulations)

for (i in 1:num_simulations) {
  X_train <- matrix(runif(n * 3, 0, 1), ncol = 3)
  epsilon <- rnorm(n)
  Y_train <- exp(X_train %*% beta) + epsilon
  knn_pred <- knn_custom(x0, X_train, Y_train, k)

  # Generate testing data
  epsilon_test <- rnorm(1)
  Y0 <- f_x0 + epsilon_test

  # Prediction error
  prediction_errors[i] <- (Y0 - knn_pred)^2
}

mean_prediction_error <- mean(prediction_errors)
mean_prediction_error
```
The prediction error is 2.424419, which is very close to the theoretical calculation 2.546811 in the previous question, which implies that the theoretical model is a good approximation of the actual behavior of the KNN model in practice.

However, the values are not exactly the same, likely due to:
- Simulation randomness: Since the simulation involves randomly generated training data and random noise, there is some inherent variation in the resulting prediction error.
- Approximation in theory: The theoretical values are estimates based on the bias and variance, which themselves were estimated from simulations.

# Question 2: Logistic Regression (30 points)

Load the library `ISLR2`. From that library, load the dataset named `Default`. Set the seed to 7 again within the chunk. Divide the dataset into a training and testing dataset. The test dataset should contain 1000 rows, the remainder should be in the training dataset.

```{r question1}
  # load library
  library(ISLR2)
  
  # load data
  data(Default)
  
  # set seed
  set.seed(7)
  
  # number of rows in entire dataset
  defaultNumRows <- dim(Default)[1]
  defaultTestNumRows <- 1000
  
  # separate dataset into train and test
  test_idx <- sample(x = 1:defaultNumRows, size = defaultTestNumRows)
  Default_train <- Default[-test_idx,]
  Default_test <- Default[test_idx,]
```

  a. [10 pts] Using the `glm()` function on the training dataset to fit a logistic regression model for the variable `default` using the input variables `balance` and `income`. Write a function called `loglikelihood` that calculates the log-likelihood for a set of coefficients (You can refer to the lecture notes). There are three input arguments for this function: a vector of coefficients (`beta`), input data matrix (`X`), and input class labels (`Y`). The output for this function is a numeric, the log likelihood (`output_loglik`). Plug in the estimated coefficients from the `glm()` model and calculate the maximum log likelihood and report it. Then, get the `deviance` value directly from the `glm()` object output. What is the relationship of deviance and maximum log likelihood?
```{r}
# Fit logistic regression model
logistic_model <- glm(default ~ balance + income, data = Default_train, family = "binomial")
loglikelihood <- function(beta, X, Y) {
  # Calculate the linear predictor
  eta <- X %*% beta
  
  # Calculate the probability of default (using the logistic function)
  p <- 1 / (1 + exp(-eta))
  
  # Calculate the log-likelihood
  output_loglik <- sum(Y * log(p) + (1 - Y) * log(1 - p))
  return(output_loglik)
}

# Extract estimated coefficients
estimated_coefficients <- coef(logistic_model)

# Prepare the input data matrix (adding intercept)
X_train <- as.matrix(cbind(1, Default_train[, c("balance", "income")]))
Y_train <- as.numeric(Default_train$default == "Yes")

# Calculate the log-likelihood using the estimated coefficients
max_log_likelihood <- loglikelihood(estimated_coefficients, X_train, Y_train)
print(max_log_likelihood)

deviance_value <- logistic_model$deviance
print(deviance_value)
```
The relationship is: deviance = -2 * maximum log likelihood.

Both deviance and log likelihood are measures of how well a model fits the data, but while log likelihood gives a direct measure, deviance provides a relative measure thatâ€™s useful for comparing models.

b. [10 pts] Use the model fit on the training dataset to estimate the probability of default for the test dataset. Use 3 different cutoff values: 0.3, 0.5, 0.7 to predict classes. For each cutoff value, print the confusion matrix. For each cutoff value, calculate and report the test error, sensitivity, specificity, and precision without using any R functions, just the addition/subtract/multiply/divide operators. Which cutoff value do you prefer in this case? If our goal is to capture as many people who will default as possible (without concerning misclassify people as `Default=Yes` even if they will not default), which cutoff value should we use?
```{r}
# Predict probabilities
predicted_probs <- predict(logistic_model, Default_test, type = "response")
# Function to calculate metrics based on cutoff
get_metrics <- function(cutoff) {
  predicted_classes <- ifelse(predicted_probs > cutoff, "Yes", "No")
  table <- table(Predicted = predicted_classes, Actual = Default_test$default)
  test_error <- mean(predicted_classes != Default_test$default)
  sensitivity <- table[2, 2] / sum(Default_test$default == "Yes")
  specificity <- table[1, 1] / sum(Default_test$default == "No")
  precision <- table[2, 2] / sum(predicted_classes == "Yes")
  list(confusion_matrix = table, test_error = test_error,
  sensitivity = sensitivity, specificity = specificity, precision = precision)
}
# Evaluate at cutoffs 0.3, 0.5, 0.7
results_0.3 <- get_metrics(0.3)
results_0.5 <- get_metrics(0.5)
results_0.7 <- get_metrics(0.7)
results_0.3
results_0.5
results_0.7
```
If our goal is general accuracy and balanced performance (with a trade-off between false positives and false negatives), the cutoff = 0.5 is preferred, as it has the lowest test error and higher precision compared to the other cutoffs, meaning it maintains a balance between identifying defaults and minimizing false positives.

If the goal is to capture as many people who will default as possible, then sensitivity is the most important metric, even at the cost of increasing false positives (misclassifying people as Default = Yes when they won't default). For this scenario, cutoff = 0.3 is the best option because it has the highest sensitivity (0.576), meaning it captures more of the actual default cases compared to the other cutoff values.

  c. [5 pts] Load the library `ROCR`. Using the functions in that library, plot the ROC curve and calculate the AUC. Use the ROC curve to determine a cutoff value and comment on your reasoning.
```{r}
# Load the library
library(ROCR)

# Prepare prediction object for ROCR
pred <- prediction(probabilities, Default_test$default)

# Create performance object for ROC
perf <- performance(pred, "tpr", "fpr")

# Plot the ROC curve
plot(perf, col = "blue", lwd = 2, main = "ROC Curve for Logistic Regression")

# Calculate AUC
auc <- performance(pred, "auc")@y.values[[1]]
print(paste("AUC:", auc))
```
A cutoff value around 0.4 is a good choice based on the ROC curve. This value provides a good balance between sensitivity and specificity, capturing a significant portion of the true positives while minimizing false positives. The ROC curve shows a steep initial increase in true positive rate (sensitivity) with a relatively low false positive rate (1-specificity) at the beginning, indicating that a lower cutoff value is effective in capturing true positives without significantly increasing false positives.

  d. [5 pts] Load the library `glmnet`. Using the `cv.glmnet()` function, do 20-fold cross-validation on the training dataset to determine the optimal penalty coefficient, $\lambda$, in the logistic regression with ridge penalty. In order to choose the best penalty coefficient use AUC as the Cross-Validation metric.
```{r}
# Load the glmnet library
library(glmnet)
# Prepare input data for glmnet
train_X_glmnet <- as.matrix(Default_train[, c("balance", "income")])
train_Y_glmnet <- Default_train$default

# Perform cross-validation to find optimal lambda
cv_fit_ridge <- cv.glmnet(train_X_glmnet, train_Y_glmnet, family = "binomial", alpha = 0, nfolds = 20, type.measure = "auc")

# Get the optimal lambda value
optimal_lambda <- cv_fit_ridge$lambda.min
print(paste("Optimal Lambda:", optimal_lambda))
```
# Question 3: K-Nearest Neighbors for Multi-class Classification (50 pts)

The MNIST dataset of handwritten digits is one of the most popular imaging data during the early times of machine learning development. Many machine learning algorithms have pushed the accuracy to over 99% on this dataset. The dataset is stored in an online repository in CSV format, `https://pjreddie.com/media/files/mnist_train.csv`. We will download the first 2500 observations of this dataset from an online resource using the following code. The first column is the digits. The remaining columns are the pixel values. After we download the dataset, we save it to our local disk so we do not have to re download the data in the future.

```{r question2}
  # inputs to download file
  fileLocation <- "https://pjreddie.com/media/files/mnist_train.csv"
  numRowsToDownload <- 2500
  localFileName <- paste0("mnist_first", numRowsToDownload, ".RData")
  
  # download the data and add column names
  mnist <- read.csv(fileLocation, nrows = numRowsToDownload)
  numColsMnist <- dim(mnist)[2]
  colnames(mnist) <- c("Digit", paste("Pixel", seq(1:(numColsMnist - 1)), sep = ""))
  
  # save file
  # in the future we can read in from the local copy instead of having to redownload
  save(mnist, file = localFileName)
  
  # you can load the data with the following code
  load(file = localFileName)
```

  a. [20 pts] The first task is to write the code to implement the K-Nearest Neighbors, or KNN, model from scratch. We will do this in steps:
  
      - Write a function called `euclidean_distance` that calculates the Euclidean distance between two vectors. There are two input arguments for this function: vector 1 (`vec1`), and vector 2 (`vec2`). The output for this function is a numeric, the Euclidean distance (`euclDist`). 
  
      - Write a function called `manhattan_distance` that calculates the Manhattan distance between two vectors. There are two input arguments for this function: vector 1 (`vec1`), and vector 2 (`vec2`). The output for this function is a numeric, the Manhattan distance (`manhDist`).
  
      - Write a function called `euclidean_distance_all` that calculates the Euclidean distance between a vector and all the row vectors in an input data matrix. There are two input arguments for this function: a vector (`vec1`) and an input data matrix (`mat1_X`). The output for this function is a vector (`output_euclDistVec`) which is of the same length as the number of rows in `mat1_X`. This function must use the function `euclidean_distance` you previously wrote.
  
      - Write a function called `manhattan_distance_all` that calculates the Manhattan distance between a vector and all the row vectors in an input data matrix. There are two input arguments for this function: a vector (`vec1`) and an input data matrix (`mat1_X`). The output for this function is a vector (`output_manhattanDistVec`) which is of the same length as the number of rows in `mat1_X`. This function must use the function `manhattan_distance` you previously wrote.
  
      - Write a function called `my_KNN` that compares a vector to a matrix and finds its K-nearest neighbors. There are five input arguments for this function: vector 1 (`vec1`), the input data matrix (`mat1_X`), the class labels corresponding to each row of the matrix (`mat1_Y`), the number of nearest neighbors you are interested in finding (`K`), and a Boolean argument specifying if we are using the Euclidean distance (`euclDistUsed`). The argument `K` should be a positive integer. If the argument `euclDistUsed = TRUE`, then use the Euclidean distance. Otherwise, use the Manhattan distance. The output of this function is a list of length 2 (`output_knnMajorityVote`). The first element in the output list should be a vector of length `K` containing the class labels of the closest neighbors. The second element in the output list should be the majority vote of the `K` class labels in the first element of the list. The function must use the functions `euclidean_distance` and `manhattan_distance` you previously wrote.  

      Apply this function to predict the label of the $123^{\text{rd}}$ observation using the first $100$ observations as your input training data matrix. Use $K = 10$. What is the predicted label when you use Euclidean distance? What is the predicted label when you use Manhattan distance? Are these predictions correct?

```{r}
euclidean_distance <- function(vec1, vec2) {
  euclDist <- sqrt(sum((vec1 - vec2) ^ 2))
  return(euclDist)
}
manhattan_distance <- function(vec1, vec2) {
  manhDist <- sum(abs(vec1 - vec2))
  return(manhDist)
}
euclidean_distance_all <- function(vec1, mat1_X) {
  output_euclDistVec <- apply(mat1_X, 1, function(row) euclidean_distance(vec1, row))
  return(output_euclDistVec)
}
manhattan_distance_all <- function(vec1, mat1_X) {
  output_manhattanDistVec <- apply(mat1_X, 1, function(row) manhattan_distance(vec1, row))
  return(output_manhattanDistVec)
}
my_KNN <- function(vec1, mat1_X, mat1_Y, K, euclDistUsed = TRUE) {
  # Calculating distances
  dist_vec <- if (euclDistUsed) {
    euclidean_distance_all(vec1, mat1_X)
  } else {
    manhattan_distance_all(vec1, mat1_X)
  }
  
  # Find the indices of the K nearest neighbors
  knn_indices <- order(dist_vec)[1:K]
  
  # Find the corresponding class labels
  knn_labels <- mat1_Y[knn_indices]
  
  # Find the majority vote among the labels
  majority_vote <- as.numeric(names(sort(table(knn_labels), decreasing = TRUE)[1]))
  
  # Return the labels and the majority vote
  output_knnMajorityVote <- list(knn_labels, majority_vote)
  return(output_knnMajorityVote)
}

train_data <- mnist[1:100, -1]  # Pixels only
train_labels <- mnist[1:100, 1] # Digits
test_data <- mnist[123, -1]     # Pixels only

knn_result_eucl <- my_KNN(test_data, train_data, train_labels, K = 10, euclDistUsed = TRUE)
eucl_predicted_label <- knn_result_eucl[[2]]
print(eucl_predicted_label)

knn_result_manh <- my_KNN(test_data, train_data, train_labels, K = 10, euclDistUsed = FALSE)
manh_predicted_label <- knn_result_manh[[2]]
print(manh_predicted_label)

actual_label <- mnist[123, 1]
print(actual_label)
```
The predicted label when using Euclidean distance is 7.
The predicted label when using Manhattan distance is also 7.
These predictions correct since they are the same as the actual label, which is 7.
  
  
  b. [20 pts] Set the seed to 7 at the beginning of the chunk. Letâ€™s now use 20-fold cross-validation to select the best $K$. Now, load the the library `caret`. We will use the `trainControl` and `train` functions from this library to fit a KNN classification model. The $K$ values we will consider are $1$, $5$, $10$, $20$, $50$, $100$. Be careful to not get confused between the number of folds and number of nearest neighbors when using the functions. Use the first $1250$ observations as the training data to fit each model. Compare the results. What is the best $K$ according to cross-validation classification accuracy? Once you have chosen $K$, fit a final KNN model on your entire training dataset with that value. Use that model to predict the classes of the last $1250$ observations, which is our test dataset. Report the prediction confusion matrix on the test dataset for your final KNN model. Calculate the the test error and the sensitivity of each classes.
```{r}
library(caret)
set.seed(7)
# Split the data into training and test sets
train_data <- mnist[1:1250, -1]  # Pixels only for training data
train_labels <- as.factor(mnist[1:1250, 1]) # Digits as factors for training labels
test_data <- mnist[1251:2500, -1]  # Pixels only for test data
test_labels <- as.factor(mnist[1251:2500, 1]) # Digits as factors for test labels

# Define training control for cross-validation
train_control <- trainControl(method = "cv", number = 20)

# Define a sequence of K values to try
k_values <- c(1, 5, 10, 20, 50, 100)

# Train KNN model with 20-fold cross-validation
knn_fit <- train(x = train_data, y = train_labels,
                 method = "knn",
                 trControl = train_control,
                 tuneGrid = expand.grid(k = k_values))
knn_fit
# Get the best K value according to cross-validation
best_k <- knn_fit$bestTune$k
print(best_k)

# Fit the final KNN model using the best K value
final_knn_model <- train(x = train_data, y = train_labels, method = "knn", tuneGrid = expand.grid(k = best_k))

# Predict on the test dataset
predictions <- predict(final_knn_model, newdata = test_data)

# Generate the confusion matrix
confusion_mat <- confusionMatrix(predictions, test_labels)

# Print the confusion matrix
print(confusion_mat)

# Calculate the test error
test_error <- 1 - confusion_mat$overall['Accuracy']
print(paste("Test Error:", test_error))

# Sensitivity for each class
sensitivity_per_class <- confusion_mat$byClass[, 'Sensitivity']
print(sensitivity_per_class)

```
The best K according to cross-validation classification accuracy is 1. 

  c. [10 pts] Set the seed to 7 at the beginning of the chunk. Now letâ€™s try to use multi-class (i.e., multinomial) logistic regression to fit the data. Use the first 1250 observations as the training data and the rest as the testing data. Load the library `glmnet`. We will use a multi-class logistic regression model with a Lasso penalty. First, we seek to find an almost optimal value for the $\lambda$ penalty parameter. Use the `cv.glmnet` function with $20$ folds on the training dataset to find $\lambda_{1se}$. Once you have identified $\lambda_{1se}$, use the `glmnet()` function with that penalty value to fit a multi-class logistic regression model onto the entire training dataset. Ensure you set the argument `family = multinomial` within the functions as appropriate. Using that model, predict the class label for the testing data. Report the testing data prediction confusion matrix. What is the test error?

```{r}
set.seed(7)
library(glmnet)
library(caret)
# Convert training and testing data to matrices
train_data <- mnist[1:1250, -1]
train_labels <- mnist[1:1250, 1]
test_data <- mnist[1251:2500, -1]
test_labels <- mnist[1251:2500, 1]
x_train <- as.matrix(train_data)
y_train <- as.factor(train_labels)
x_test <- as.matrix(test_data)
y_test <- as.factor(test_labels)

# Perform cross-validation to find the optimal lambda
cv_fit <- cv.glmnet(x_train, y_train, family = "multinomial", type.measure = "class", alpha = 1, nfolds = 20)

lambda_1se <- cv_fit$lambda.1se
cat("Lambda at one standard error:", lambda_1se, "\n")

# Get the sequence of lambda
lambda_sequence <- cv_fit$lambda
# Fit the final model using the lambda.1se
final_model <- glmnet(x_train, y_train, family = "multinomial", lambda = lambda_sequence, alpha = 1)
# Predict class labels for the test data
predictions <- predict(final_model, newx = x_test, s = lambda_1se, type = "class")
# Convert predictions to factor to match test labels
predictions <- as.factor(predictions)
# Create the confusion matrix
final_conf_matrix <- confusionMatrix(predictions, y_test)
cat("The confusion matrix:\n")
print(final_conf_matrix)
# Calculate the test error
final_test_error <- 1 - sum(diag(final_conf_matrix$table)) / sum(final_conf_matrix$table)
cat("Test Error:", final_test_error, "\n")
```