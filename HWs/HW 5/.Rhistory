# Print results in matrix format
cat("Cutoff:", cutoff, "\n")
cat("Confusion Matrix:\n")
confusion_matrix <- matrix(c(TP, FP, FN, TN), nrow = 2, byrow = TRUE,
dimnames = list("Predicted" = c("Yes", "No"),
"Actual" = c("Yes", "No")))
print(confusion_matrix)
# Print metrics
cat("Test Error:", test_error, "\n")
cat("Sensitivity:", sensitivity, "\n")
cat("Specificity:", specificity, "\n")
cat("Precision:", precision, "\n\n")
# Return a list of metrics
return(list(test_error = test_error, sensitivity = sensitivity, specificity = specificity, precision = precision))
}
# Apply the function for cutoff values 0.3, 0.5, and 0.7
metrics_0.3 <- calculate_metrics(probabilities, Default_test$default, 0.3)
metrics_0.5 <- calculate_metrics(probabilities, Default_test$default, 0.5)
metrics_0.7 <- calculate_metrics(probabilities, Default_test$default, 0.7)
calculate_metrics <- function(probabilities, actual, cutoff) {
# Convert probabilities to predicted classes based on cutoff
predicted <- ifelse(probabilities >= cutoff, "Yes", "No")
# Convert actual classes to binary representation
actual <- ifelse(actual == "Yes", "Yes", "No")
# Calculate confusion matrix elements
TP <- sum(predicted == "Yes" & actual == "Yes")
TN <- sum(predicted == "No" & actual == "No")
FP <- sum(predicted == "Yes" & actual == "No")
FN <- sum(predicted == "No" & actual == "Yes")
# Calculate metrics
test_error <- (FP + FN) / length(actual)
sensitivity <- TP / (TP + FN)
specificity <- TN / (TN + FP)
precision <- TP / (TP + FP)
# Print results in matrix format
cat("Cutoff:", cutoff, "\n")
cat("Confusion Matrix:\n")
confusion_matrix <- matrix(c(TP, FP, FN, TN), nrow = 2, byrow = TRUE,
dimnames = list("Predicted" = c("Yes", "No"),
"Actual" = c("Yes", "No")))
print(confusion_matrix)
# Print metrics
cat("Test Error:", test_error, "\n")
cat("Sensitivity:", sensitivity, "\n")
cat("Specificity:", specificity, "\n")
cat("Precision:", precision, "\n\n")
# Return a list of metrics
return(list(test_error = test_error, sensitivity = sensitivity, specificity = specificity, precision = precision))
}
# Apply the function for cutoff values 0.3, 0.5, and 0.7
metrics_0.3 <- calculate_metrics(probabilities, Default_test$default, 0.3)
metrics_0.5 <- calculate_metrics(probabilities, Default_test$default, 0.5)
metrics_0.7 <- calculate_metrics(probabilities, Default_test$default, 0.7)
# Chunk 1
.solution {
calculate_metrics <- function(probabilities, actual, cutoff) {
# Convert probabilities to predicted classes based on cutoff
predicted <- ifelse(probabilities >= cutoff, "Yes", "No")
# Convert actual classes to binary representation
actual <- ifelse(actual == "Yes", "Yes", "No")
# Calculate confusion matrix elements
TP <- sum(predicted == "Yes" & actual == "Yes")
TN <- sum(predicted == "No" & actual == "No")
FP <- sum(predicted == "Yes" & actual == "No")
FN <- sum(predicted == "No" & actual == "Yes")
# Calculate metrics
test_error <- (FP + FN) / length(actual)
sensitivity <- TP / (TP + FN)
specificity <- TN / (TN + FP)
precision <- TP / (TP + FP)
# Print results in matrix format
cat("Cutoff:", cutoff, "\n")
cat("Confusion Matrix:\n")
confusion_matrix <- matrix(c(TP, FP, FN, TN), nrow = 2, byrow = TRUE,
dimnames = list("Predicted" = c("Yes", "No"),
"Actual" = c("Yes", "No")))
print(confusion_matrix)
# Print metrics
cat("Test Error:", test_error, "\n")
cat("Sensitivity:", sensitivity, "\n")
cat("Specificity:", specificity, "\n")
cat("Precision:", precision, "\n\n")
# Return a list of metrics
return(list(test_error = test_error, sensitivity = sensitivity, specificity = specificity, precision = precision))
}
# Apply the function for cutoff values 0.3, 0.5, and 0.7
metrics_0.3 <- calculate_metrics(probabilities, Default_test$default, 0.3)
metrics_0.5 <- calculate_metrics(probabilities, Default_test$default, 0.5)
metrics_0.7 <- calculate_metrics(probabilities, Default_test$default, 0.7)
# Predict probabilities
predicted_probs <- predict(model, Default_test, type = "response")
# Predict probabilities
predicted_probs <- predict(logistic_model, Default_test, type = "response")
# Function to calculate metrics based on cutoff
get_metrics <- function(cutoff) {
predicted_classes <- ifelse(predicted_probs > cutoff, "Yes", "No")
table <- table(Predicted = predicted_classes, Actual = Default_test$default)
test_error <- mean(predicted_classes != Default_test$default)
sensitivity <- table[2, 2] / sum(Default_test$default == "Yes")
specificity <- table[1, 1] / sum(Default_test$default == "No")
precision <- table[2, 2] / sum(predicted_classes == "Yes")
list(confusion_matrix = table, test_error = test_error,
sensitivity = sensitivity, specificity = specificity, precision = precision)
}
# Evaluate at cutoffs 0.3, 0.5, 0.7
results_0.3 <- get_metrics(0.3)
results_0.5 <- get_metrics(0.5)
results_0.7 <- get_metrics(0.7)
results_0.3
# Predict probabilities
predicted_probs <- predict(logistic_model, Default_test, type = "response")
# Function to calculate metrics based on cutoff
get_metrics <- function(cutoff) {
predicted_classes <- ifelse(predicted_probs > cutoff, "Yes", "No")
table <- table(Predicted = predicted_classes, Actual = Default_test$default)
test_error <- mean(predicted_classes != Default_test$default)
sensitivity <- table[2, 2] / sum(Default_test$default == "Yes")
specificity <- table[1, 1] / sum(Default_test$default == "No")
precision <- table[2, 2] / sum(predicted_classes == "Yes")
list(confusion_matrix = table, test_error = test_error,
sensitivity = sensitivity, specificity = specificity, precision = precision)
}
# Evaluate at cutoffs 0.3, 0.5, 0.7
results_0.3 <- get_metrics(0.3)
results_0.5 <- get_metrics(0.5)
results_0.7 <- get_metrics(0.7)
results_0.3
results_0.5
results_0.7
beta <- c(0.5, -0.5, 0)
x0 <- c(1, -0.75, -0.7)
f_x0 <- exp(sum(beta * x0))
f_x0
#install.packages("FNN")
library(FNN)
set.seed(123)
n <- 100
X_train <- matrix(runif(n * 3, 0, 1), ncol = 3)
epsilon <- rnorm(n)
Y_train <- exp(X_train %*% beta) + epsilon
knn_custom <- function(x0, X_train, Y_train, k) {
distances <- apply(X_train, 1, function(row) sqrt(sum((row - x0)^2)))
nearest_indices <- order(distances)[1:k]
mean(Y_train[nearest_indices])
}
k <- 21
knn_pred <- knn_custom(x0, X_train, Y_train, k)
knn_pred
#validate with knn.reg
knn_pred_validation <- knn.reg(X_train, test = matrix(x0, nrow = 1), y = Y_train, k = k)$pred
knn_pred_validation
set.seed(123)
num_simulations <- 1000
knn_preds <- numeric(num_simulations)
for (i in 1:num_simulations) {
X_train <- matrix(runif(n * 3, 0, 1), ncol = 3)
epsilon <- rnorm(n)
Y_train <- exp(X_train %*% beta) + epsilon
knn_preds[i] <- knn_custom(x0, X_train, Y_train, k)
}
estimated_bias <- mean(knn_preds) - f_x0
cat("The Bias:", estimated_bias, "\n")
estimated_variance <- var(knn_preds)
cat("The Variance:", estimated_variance, "\n")
prediction_error <- 1 + (estimated_bias^2) + estimated_variance
cat("The Calculated Prediction Error:", prediction_error, "\n")
prediction_errors <- numeric(num_simulations)
for (i in 1:num_simulations) {
X_train <- matrix(runif(n * 3, 0, 1), ncol = 3)
epsilon <- rnorm(n)
Y_train <- exp(X_train %*% beta) + epsilon
knn_pred <- knn_custom(x0, X_train, Y_train, k)
# Generate testing data
epsilon_test <- rnorm(1)
Y0 <- f_x0 + epsilon_test
# Prediction error
prediction_errors[i] <- (Y0 - knn_pred)^2
}
mean_prediction_error <- mean(prediction_errors)
mean_prediction_error
# load library
library(ISLR2)
# load data
data(Default)
# set seed
set.seed(7)
# number of rows in entire dataset
defaultNumRows <- dim(Default)[1]
defaultTestNumRows <- 1000
# separate dataset into train and test
test_idx <- sample(x = 1:defaultNumRows, size = defaultTestNumRows)
Default_train <- Default[-test_idx,]
Default_test <- Default[test_idx,]
# Fit logistic regression model
logistic_model <- glm(default ~ balance + income, data = Default_train, family = "binomial")
loglikelihood <- function(beta, X, Y) {
# Calculate the linear predictor
eta <- X %*% beta
# Calculate the probability of default (using the logistic function)
p <- 1 / (1 + exp(-eta))
# Calculate the log-likelihood
output_loglik <- sum(Y * log(p) + (1 - Y) * log(1 - p))
return(output_loglik)
}
# Extract estimated coefficients
estimated_coefficients <- coef(logistic_model)
# Prepare the input data matrix (adding intercept)
X_train <- as.matrix(cbind(1, Default_train[, c("balance", "income")]))
Y_train <- as.numeric(Default_train$default == "Yes")
# Calculate the log-likelihood using the estimated coefficients
max_log_likelihood <- loglikelihood(estimated_coefficients, X_train, Y_train)
print(max_log_likelihood)
deviance_value <- logistic_model$deviance
print(deviance_value)
# Predict the probability of default on the test dataset
probabilities <- predict(logistic_model, newdata = Default_test, type = "response")
calculate_metrics <- function(probabilities, actual, cutoff) {
# Convert probabilities to predicted classes based on cutoff
predicted <- ifelse(probabilities >= cutoff, "Yes", "No")
# Convert actual classes to binary representation
actual <- ifelse(actual == "Yes", "Yes", "No")
# Calculate confusion matrix elements
TP <- sum(predicted == "Yes" & actual == "Yes")
TN <- sum(predicted == "No" & actual == "No")
FP <- sum(predicted == "Yes" & actual == "No")
FN <- sum(predicted == "No" & actual == "Yes")
# Calculate metrics
test_error <- (FP + FN) / length(actual)
sensitivity <- TP / (TP + FN)
specificity <- TN / (TN + FP)
precision <- TP / (TP + FP)
# Print results
cat("Cutoff:", cutoff, "\n")
cat("Confusion Matrix:\n")
cat("TP:", TP, "FP:", FP, "TN:", TN, "FN:", FN, "\n")
cat("Test Error:", test_error, "\n")
cat("Sensitivity:", sensitivity, "\n")
cat("Specificity:", specificity, "\n")
cat("Precision:", precision, "\n\n")
# Return a list of metrics
return(list(test_error = test_error, sensitivity = sensitivity, specificity = specificity, precision = precision))
}
# Apply the function for cutoff values 0.3, 0.5, and 0.7
metrics_0.3 <- calculate_metrics(probabilities, Default_test$default, 0.3)
metrics_0.5 <- calculate_metrics(probabilities, Default_test$default, 0.5)
metrics_0.7 <- calculate_metrics(probabilities, Default_test$default, 0.7)
# Predict the probability of default on the test dataset
probabilities <- predict(logistic_model, newdata = Default_test, type = "response")
calculate_metrics <- function(probabilities, actual, cutoff) {
# Convert probabilities to predicted classes based on cutoff
predicted <- ifelse(probabilities >= cutoff, "Yes", "No")
# Convert actual classes to binary representation
actual <- ifelse(actual == "Yes", "Yes", "No")
# Calculate confusion matrix elements
TP <- sum(predicted == "Yes" & actual == "Yes")
TN <- sum(predicted == "No" & actual == "No")
FP <- sum(predicted == "Yes" & actual == "No")
FN <- sum(predicted == "No" & actual == "Yes")
# Create and print confusion matrix in matrix format
confusion_matrix <- matrix(c(TP, FP, FN, TN), nrow = 2, byrow = TRUE,
dimnames = list("Predicted" = c("Yes", "No"),
"Actual" = c("Yes", "No")))
cat("Cutoff:", cutoff, "\n")
cat("Confusion Matrix:\n")
print(confusion_matrix)
# Calculate metrics
test_error <- (FP + FN) / length(actual)
sensitivity <- TP / (TP + FN)
specificity <- TN / (TN + FP)
precision <- TP / (TP + FP)
# Print metrics
cat("Test Error:", test_error, "\n")
cat("Sensitivity:", sensitivity, "\n")
cat("Specificity:", specificity, "\n")
cat("Precision:", precision, "\n\n")
# Return a list of metrics
return(list(test_error = test_error, sensitivity = sensitivity, specificity = specificity, precision = precision))
}
# Apply the function for cutoff values 0.3, 0.5, and 0.7
metrics_0.3 <- calculate_metrics(probabilities, Default_test$default, 0.3)
metrics_0.5 <- calculate_metrics(probabilities, Default_test$default, 0.5)
metrics_0.7 <- calculate_metrics(probabilities, Default_test$default, 0.7)
# Load the library
library(ROCR)
# Prepare prediction object for ROCR
pred <- prediction(probabilities, Default_test$default)
# Create performance object for ROC
perf <- performance(pred, "tpr", "fpr")
# Plot the ROC curve
plot(perf, col = "blue", lwd = 2, main = "ROC Curve for Logistic Regression")
# Load the library
library(ROCR)
# Prepare prediction object for ROCR
pred <- prediction(probabilities, Default_test$default)
# Predictions object
pred <- prediction(predicted_probs, Default_test$default)
# Load the library
library(ROCR)
# Prepare prediction object for ROCR
pred <- prediction(predicted_probs, Default_test$default)
library(ROCR)
probabilities <- predict(glm_model, newdata = Default_test, type = "response")
# Predict the probability of default on the test dataset
probabilities <- predict(logistic_model, newdata = Default_test, type = "response")
calculate_metrics <- function(probabilities, actual, cutoff) {
# Convert probabilities to predicted classes based on cutoff
predicted <- ifelse(probabilities >= cutoff, "Yes", "No")
# Convert actual classes to binary representation
actual <- ifelse(actual == "Yes", "Yes", "No")
# Calculate confusion matrix elements
TP <- sum(predicted == "Yes" & actual == "Yes")
TN <- sum(predicted == "No" & actual == "No")
FP <- sum(predicted == "Yes" & actual == "No")
FN <- sum(predicted == "No" & actual == "Yes")
# Create and print confusion matrix in matrix format
confusion_matrix <- matrix(c(TP, FP, FN, TN), nrow = 2, byrow = TRUE,
dimnames = list("Predicted" = c("Yes", "No"),
"Actual" = c("Yes", "No")))
cat("Cutoff:", cutoff, "\n")
cat("Confusion Matrix:\n")
print(confusion_matrix)
# Calculate metrics
test_error <- (FP + FN) / length(actual)
sensitivity <- TP / (TP + FN)
specificity <- TN / (TN + FP)
precision <- TP / (TP + FP)
# Print metrics
cat("Test Error:", test_error, "\n")
cat("Sensitivity:", sensitivity, "\n")
cat("Specificity:", specificity, "\n")
cat("Precision:", precision, "\n\n")
# Return a list of metrics
return(list(test_error = test_error, sensitivity = sensitivity, specificity = specificity, precision = precision))
}
# Apply the function for cutoff values 0.3, 0.5, and 0.7
metrics_0.3 <- calculate_metrics(probabilities, Default_test$default, 0.3)
metrics_0.5 <- calculate_metrics(probabilities, Default_test$default, 0.5)
metrics_0.7 <- calculate_metrics(probabilities, Default_test$default, 0.7)
library(ROCR)
pred <- prediction(probabilities, Default_test$default)
# Performance object for ROC curve
perf <- performance(pred, "tpr", "fpr")
# Plot ROC curve
plot(perf, colorize = TRUE, main = "ROC Curve", xlab = "False Positive Rate", ylab = "True Positive Rate")
library(ROCR)
pred <- prediction(probabilities, Default_test$default)
# Performance object for ROC curve
perf <- performance(pred, "tpr", "fpr")
# Plot ROC curve
par(mar = c(4, 4, 2, 2))
plot(perf, colorize = TRUE, main = "ROC Curve", xlab = "False Positive Rate", ylab = "True Positive Rate")
par(mfrow = c(1, 1))  # Set to a single plot layout
plot(perf, colorize = TRUE, main = "ROC Curve", xlab = "False Positive Rate", ylab = "True Positive Rate")
par(mar = c(4, 4, 2, 2))  # Adjust the margins to make them smaller
plot(perf, colorize = TRUE, main = "ROC Curve", xlab = "False Positive Rate", ylab = "True Positive Rate")
par(mar = c(4, 4, 2, 2))  # Reduce the margin size
plot(perf, colorize = TRUE, main = "ROC Curve", xlab = "False Positive Rate", ylab = "True Positive Rate")
par(mfrow = c(1, 1))  # Set to a single plot layout
plot(perf, colorize = TRUE, main = "ROC Curve", xlab = "False Positive Rate", ylab = "True Positive Rate")
# Save plot to a PNG file with larger dimensions
png("roc_curve.png", width = 800, height = 600)
plot(perf, colorize = TRUE, main = "ROC Curve", xlab = "False Positive Rate", ylab = "True Positive Rate")
abline(a = 0, b = 1, col = "gray", lty = 2)  # Diagonal reference line
dev.off()
# Calculate AUC
auc_perf <- performance(pred, measure = "auc")
auc_value <- auc_perf@y.values[[1]]
auc_value
# Load the glmnet library
library(glmnet)
# Prepare input data for glmnet
train_X_glmnet <- as.matrix(Default_train[, c("balance", "income")])
train_Y_glmnet <- Default_train$default
# Perform cross-validation to find optimal lambda
cv_fit_ridge <- cv.glmnet(train_X_glmnet, train_Y_glmnet, family = "binomial", alpha = 0, nfolds = 20, type.measure = "auc")
# Get the optimal lambda value
optimal_lambda <- cv_fit_ridge$lambda.min
print(paste("Optimal Lambda:", optimal_lambda))
plot(cv_fit_ridge)
# Load the glmnet library
library(glmnet)
# Prepare input data for glmnet
train_X_glmnet <- as.matrix(Default_train[, c("balance", "income")])
train_Y_glmnet <- Default_train$default
# Perform cross-validation to find optimal lambda
cv_fit_ridge <- cv.glmnet(train_X_glmnet, train_Y_glmnet, family = "binomial", alpha = 0, nfolds = 20, type.measure = "auc")
plot(cv_fit_ridge)
# Get the optimal lambda value
optimal_lambda <- cv_fit_ridge$lambda.min
# Best AUC corresponding to the best lambda
best_auc <- max(cv_model$cvm)
print(paste("Optimal Lambda:", optimal_lambda))
# Best AUC corresponding to the best lambda
best_auc <- max(cv_model$cvm)
# Best AUC corresponding to the best lambda
best_auc <- max(cv_fit_ridgel$cvm)
# Best AUC corresponding to the best lambda
best_auc <- max(cv_fit_ridge$cvm)
best_auc
# Perform cross-validation to find optimal lambda
cv_fit_ridge <- cv.glmnet(train_X_glmnet, train_Y_glmnet, family = "binomial", alpha = 0, nfolds = 20, type.measure = "auc")
#install.packages("FNN")
library(FNN)
set.seed(123)
n <- 100
X_train <- matrix(runif(n * 3, 0, 1), ncol = 3)
epsilon <- rnorm(n)
Y_train <- exp(X_train %*% beta) + epsilon
knn_custom <- function(x0, X_train, Y_train, k) {
distances <- apply(X_train, 1, function(row) sqrt(sum((row - x0)^2)))
nearest_indices <- order(distances)[1:k]
mean(Y_train[nearest_indices])
}
k <- 21
knn_pred <- knn_custom(x0, X_train, Y_train, k)
knn_pred
#validate with knn.reg
knn_pred_validation <- knn.reg(X_train, test = matrix(x0, nrow = 1), y = Y_train, k = k)$pred
knn_pred_validation
set.seed(123)
num_simulations <- 1000
knn_preds <- numeric(num_simulations)
for (i in 1:num_simulations) {
X_train <- matrix(runif(n * 3, 0, 1), ncol = 3)
epsilon <- rnorm(n)
Y_train <- exp(X_train %*% beta) + epsilon
knn_preds[i] <- knn_custom(x0, X_train, Y_train, k)
}
estimated_bias <- mean(knn_preds) - f_x0
cat("The Bias:", estimated_bias, "\n")
estimated_variance <- var(knn_preds)
cat("The Variance:", estimated_variance, "\n")
prediction_error <- 1 + (estimated_bias^2) + estimated_variance
cat("The Calculated Prediction Error:", prediction_error, "\n")
prediction_errors <- numeric(num_simulations)
for (i in 1:num_simulations) {
X_train <- matrix(runif(n * 3, 0, 1), ncol = 3)
epsilon <- rnorm(n)
Y_train <- exp(X_train %*% beta) + epsilon
knn_pred <- knn_custom(x0, X_train, Y_train, k)
# Generate testing data
epsilon_test <- rnorm(1)
Y0 <- f_x0 + epsilon_test
# Prediction error
prediction_errors[i] <- (Y0 - knn_pred)^2
}
mean_prediction_error <- mean(prediction_errors)
mean_prediction_error
# Generate testing data and calculate the empirical prediction error
test_epsilon <- rnorm(1000)
test_y0 <- exp(beta_x0) + test_epsilon
prediction_error <- 1 + (estimated_bias^2) + estimated_variance
cat("The Calculated Prediction Error:", prediction_error, "\n")
prediction_errors <- numeric(num_simulations)
for (i in 1:num_simulations) {
X_train <- matrix(runif(n * 3, 0, 1), ncol = 3)
epsilon <- rnorm(n)
Y_train <- exp(X_train %*% beta) + epsilon
knn_pred <- knn_custom(x0, X_train, Y_train, k)
# Generate testing data
epsilon_test <- rnorm(1)
Y0 <- f_x0 + epsilon_test
# Prediction error
prediction_errors[i] <- (Y0 - knn_pred)^2
}
mean_prediction_error <- mean(prediction_errors)
mean_prediction_error
set.seed(123)
prediction_errors <- numeric(num_simulations)
for (i in 1:num_simulations) {
X_train <- matrix(runif(n * 3, 0, 1), ncol = 3)
epsilon <- rnorm(n)
Y_train <- exp(X_train %*% beta) + epsilon
knn_pred <- knn_custom(x0, X_train, Y_train, k)
# Generate testing data
epsilon_test <- rnorm(1)
Y0 <- f_x0 + epsilon_test
# Prediction error
prediction_errors[i] <- (Y0 - knn_pred)^2
}
mean_prediction_error <- mean(prediction_errors)
mean_prediction_error
beta <- c(0.5, -0.5, 0)
x0 <- c(1, -0.75, -0.7)
f_x0 <- exp(sum(beta * x0))
cat("true mean of Y at \(x_0\)", f_x0, "\n")
beta <- c(0.5, -0.5, 0)
x0 <- c(1, -0.75, -0.7)
f_x0 <- exp(sum(beta * x0))
cat("true mean of Y at $(x_0\$", f_x0, "\n")
beta <- c(0.5, -0.5, 0)
x0 <- c(1, -0.75, -0.7)
f_x0 <- exp(sum(beta * x0))
cat("true mean of Y at testing point: ", f_x0, "\n")
#install.packages("FNN")
library(FNN)
set.seed(123)
n <- 100
X_train <- matrix(runif(n * 3, 0, 1), ncol = 3)
epsilon <- rnorm(n)
Y_train <- exp(X_train %*% beta) + epsilon
knn_custom <- function(x0, X_train, Y_train, k) {
distances <- apply(X_train, 1, function(row) sqrt(sum((row - x0)^2)))
nearest_indices <- order(distances)[1:k]
mean(Y_train[nearest_indices])
}
k <- 21
knn_pred <- knn_custom(x0, X_train, Y_train, k)
knn_pred
#validate with knn.reg
knn_pred_validation <- knn.reg(X_train, test = matrix(x0, nrow = 1), y = Y_train, k = k)$pred
knn_pred_validation
