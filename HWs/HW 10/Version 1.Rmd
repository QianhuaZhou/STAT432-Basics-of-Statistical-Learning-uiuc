---
title: "Stat 432 Homework 10"
date: "Assigned: Oct 28, 2024; <span style='color:red'>Due: 11:59 PM CT, Nov 7, 2024</span>"
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
editor_options: 
  chunk_output_type: console
---

<style>
  body {
  text-align: justify}
</style>

```{css, echo=FALSE}
  .solution {
  background-color: #CCDDFF; /* Replace with your desired color */
  }
  
  blockquote {
  background-color: #CCDDFF; /* Replace with your desired color */
  font-family: "Times New Roman", serif; /* Change to your desired font family */
  /* font-size: 16px; /* Change to your desired font size */
  font-weight: bold; /* Makes the text bold */
  }
```

# Question 1: K-means Clustering [65 pts]

In this question, we will code our own k-means clustering algorithm. The __key requirement__ is that you __cannot write your code directly__. You __must write a proper prompt__ to describe your intention for each of the function so that GPT (or whatever AI tools you are using) can understand your way of thinking clearly, and provide you with the correct code. We will use the handwritten digits dataset from HW9 (2600 observations). Recall that the k-means algorithm iterates between two steps:

  + Assign each observation to the cluster with the closest centroid.
  + Update the centroids to be the mean of the observations assigned to each cluster.
  
You do not need to split the data into train and test. We will use the whole dataset as the training data. Restrict the data to just the digits 2, 4 and 8. And then perform marginal variance screening to __reduce to the top 50__ features. After this, complete the following tasks. Please read all sub-questions a, b, and c before you start, and think about how different pieces of the code should be structured and what the inputs and outputs should be so that they can be integrated. For each question, you need to document your prompt to GPT (or whatever AI tools you are using) to generate the code. __You cannot wirte your own code or modify the code generated by the AI tool in any of the function definitions.__
```{r}
  # inputs to download file
  fileLocation <- "https://pjreddie.com/media/files/mnist_train.csv"
  numRowsToDownload <- 2600
  localFileName <- paste0("mnist_first", numRowsToDownload, ".RData")
  
  # download the data and add column names
  mnist2600 <- read.csv(fileLocation, nrows = numRowsToDownload)
  numColsMnist <- dim(mnist2600)[2]
  colnames(mnist2600) <- c("Digit", paste("Pixel", seq(1:(numColsMnist - 1)), sep = ""))
  
  # save file
  # in the future we can read in from the local copy instead of having to redownload
  save(mnist2600, file = localFileName)

  # you can load the data with the following code
  #load(file = localFileName)
  dim(mnist2600)
  # Subset data to include only digits 2, 4, and 8
  mnist <- mnist2600[mnist2600$Digit %in% c(2, 4, 8), ]
  # Calculate variance for each pixel column
  pixel_vars <- apply(mnist[ , -1], 2, var)
  # Select top 250 pixel columns with the highest variance
  top_pixels <- names(sort(pixel_vars, decreasing = TRUE))[1:50]
  # Subset training and test sets to only include these top 250 pixels
  num_top_50 <- mnist[, c("Digit", top_pixels)]
```

  a. [20 pts] In this question, we want to ask GPT to write a function called `cluster_mean_update()` that takes in three arguments, the data $X$, the number of clusters $K$, and the cluster assignments. And it outputs the updated centroids. Think about how you should describe the task to GPT (your specific requirements of how these arguments and the output should structured) so that it can understand your intention. You need to request the AI tool to provide sufficient comments for each step of the function. After this, test your function with the training data, $K = 3$ and a random cluster assignment. 
#### Prompt
Write a function called `cluster_mean_update()` that takes in three arguments, the data $X$, the number of clusters $K$, and the cluster assignments.Outputs the updated centroids. 

```{r}
cluster_mean_update <- function(X, K, cluster_assignments) {
  # Initialize a matrix to store the updated centroids
  centroids <- matrix(0, nrow = K, ncol = ncol(X))
  
  # Loop through each cluster to calculate the mean for each one
  for (k in 1:K) {
    # Subset the data for the current cluster
    cluster_data <- X[cluster_assignments == k, ]
    
    # Calculate the mean of each column (i.e., each pixel) for the current cluster
    centroids[k, ] <- colMeans(cluster_data)
  }
  
  return(centroids)
}
```
#### Testing the cluster_mean_update Function
```{r}
set.seed(123)  # Set seed for reproducibility
K <- 3
# Generate random cluster assignments
random_cluster_assignments <- sample(1:K, nrow(num_top_50), replace = TRUE)

# Get the updated centroids
centroids <- cluster_mean_update(num_top_50[, -1], K, random_cluster_assignments)
print(centroids)
```
\bigskip

  b. [20 pts] Next, we want to ask GPT to write a function called `cluster_assignments()` that takes in two arguments, the data $X$ and the centroids. And it outputs the cluster assignments. Think about how you should describe the task to GPT so that this function would be compatible with the previous function to achieve the k-means clustering. You need to request the AI tool to provide sufficient comments for each step of the function. After this, test your function with the training data and the centroids from the previous step.
#### Prompt
Write a function called `cluster_assignments()` that takes in two arguments, the data $X$ and the centroids. Outputs the cluster assignments.
```{r}
cluster_assignments <- function(X, centroids) {
  # Initialize a vector to store the cluster assignments
  assignments <- integer(nrow(X))
  
  # Loop through each data point
  for (i in 1:nrow(X)) {
    # Calculate the Euclidean distance between the data point and each centroid
    distances <- apply(centroids, 1, function(centroid) sum((X[i, ] - centroid)^2))
    
    # Assign the data point to the closest centroid
    assignments[i] <- which.min(distances)
  }
  
  return(assignments)
}
```
#### Testing the cluster_assignments Function
```{r}
# Use the centroids from the previous test
assignments <- cluster_assignments(num_top_50[, -1], centroids)
print(assignments)
```

\bigskip

  c. [20 pts] Finally, we want to ask GPT to write a function called `kmeans()`. What arguments should you supply? And what outputs should be requested? Again, think about how you should describe the task to GPT. Test your function with the training data, $K = 3$, and the maximum number of iterations set to 20. For this code, you can skip the multiple starting points strategy. However, keep in mind that your solution maybe suboptimal.
#### Prompt
Write a function called `kmeans()` that takes in three arguments, the data $X$, the number of clusters $K$, and the maximum number of iterations. Outputs the cluster assignments and the final centroids.
```{r}
kmeans <- function(X, K, max_iters = 100) {
  # Step 1: Randomly initialize K centroids by selecting K random rows from X
  set.seed(123)  # Set seed for reproducibility
  initial_centroids <- X[sample(1:nrow(X), K), ]
  
  # Initialize cluster assignments
  cluster_assignments <- rep(0, nrow(X))
  
  for (iter in 1:max_iters) {
    # Step 2: Assign points to the nearest centroid
    cluster_assignments <- cluster_assignments(X, initial_centroids)
    
    # Step 3: Update centroids based on current assignments
    new_centroids <- cluster_mean_update(X, K, cluster_assignments)
    
    # Check for convergence (if centroids do not change)
    if (all(initial_centroids == new_centroids)) {
      cat("Converged in", iter, "iterations.\n")
      break
    }
    
    # Update centroids for the next iteration
    initial_centroids <- new_centroids
  }
  
  # Return the final cluster assignments and centroids
  list(cluster_assignments = cluster_assignments, centroids = initial_centroids)
}
```
#### Testing the kmeans Function
```{r}
# Run k-means clustering on num_top_50 with K = 3 and max_iters = 20
kmeans_result <- kmeans(num_top_50[, -1], K = 3, max_iters = 20)

# Print the final cluster assignments and centroids
print(kmeans_result$cluster_assignments)
print(kmeans_result$centroids)
```

\bigskip

  d. [5 pts] After completing the above tasks, check your clustering results with the true labels in the training dataset. Is your code working as expected? What is the accuracy of the clustering? You are not restricted to use the AI tool from now on. Comment on whether you think the code generated by GPT can be improved (in any ways).
```{r}
# True labels in the dataset
true_labels <- num_top_50$Digit

# Get the predicted cluster assignments from k-means results
predicted_clusters <- kmeans_result$cluster_assignments

# Create a mapping between clusters and true labels
label_mapping <- sapply(1:K, function(cluster) {
  # Find the true label that is most frequent in each cluster
  majority_label <- names(sort(table(true_labels[predicted_clusters == cluster]), decreasing = TRUE))[1]
  return(as.numeric(majority_label))
})

# Map each predicted cluster to the majority true label
predicted_labels <- sapply(predicted_clusters, function(cluster) label_mapping[cluster])

# Calculate accuracy
accuracy <- sum(predicted_labels == true_labels) / length(true_labels)
cat("Clustering Accuracy:", round(accuracy * 100, 2), "%\n")
```

\bigskip

# Question 2: Hierarchical Clustering

In this question, we will use the hierarchical clustering algorithm to cluster the training data. We will use the same training data as in Question 1. Directly use the `hclust()` function in R to perform hierarchical clustering, but test different linkage methods (single, complete, and average) and euclidean distance. 

  a. [10 pts] Plot the three dendrograms and compare them. What do you observe? Which linkage method do you think is the most appropriate for this dataset?
```{r}
# Calculate the Euclidean distance matrix
dist_matrix <- dist(num_top_50, method = "euclidean")

# Perform hierarchical clustering with different linkage methods
hc_single <- hclust(dist_matrix, method = "single")
hc_complete <- hclust(dist_matrix, method = "complete")
hc_average <- hclust(dist_matrix, method = "average")

# Plot the dendrograms
par(mfrow = c(1, 3))  # Arrange plots side by side
plot(hc_single, main = "Single Linkage", xlab = "", sub = "", cex = 0.6)
plot(hc_complete, main = "Complete Linkage", xlab = "", sub = "", cex = 0.6)
plot(hc_average, main = "Average Linkage", xlab = "", sub = "", cex = 0.6)
par(mfrow = c(1, 1))  # Reset plot layout
```
Single Linkage:
- The dendrogram is highly elongated with many "chained" clusters, where individual data points are linked one-by-one.
- This chaining effect makes it harder to distinguish clear, well-separated clusters.
- Single linkage is generally sensitive to noise and outliers, which can result in this kind of chaining pattern.

Complete Linkage:
- The dendrogram is more balanced and compact, with clusters that appear well-separated.
- This linkage method tends to produce more spherical clusters, which is often desirable if the data has distinct groups.
- Complete linkage minimizes the maximum distance between clusters, making it less sensitive to outliers and better suited for identifying well-defined clusters.

Average Linkage:
- The average linkage dendrogram is somewhat between single and complete linkage in terms of cluster compactness.
- Clusters are generally balanced, though not as tight as with complete linkage.

Based on the structure observed in the dendrograms, complete linkage seems to be the most appropriate for this dataset. It provides well-separated, compact clusters that are less prone to the chaining effect seen in single linkage, making it easier to interpret the clustering structure.



\bigskip
  b. [10 pts] Choose your linkage method, cut the dendrogram to obtain 3 clusters and compare the clustering results with the true labels in the training dataset. What is the accuracy of the clustering? Comment on its performance. 
```{r}
# Perform complete linkage clustering on the distance matrix
hc_complete <- hclust(dist_matrix, method = "complete")

# Cut the dendrogram to obtain 3 clusters
clusters <- cutree(hc_complete, k = 3)

# Create a contingency table between true labels and predicted cluster labels
table_result <- table(true_labels, clusters)

# Map each cluster label to the most frequent true label within that cluster
cluster_to_label <- sapply(1:ncol(table_result), function(col) {
  true_label <- names(which.max(table_result[, col]))
  return(true_label)
})

# Convert the mapping result to a named character vector
names(cluster_to_label) <- colnames(table_result)

# Replace cluster labels with the mapped labels
mapped_labels <- as.character(cluster_to_label[as.character(clusters)])

# Calculate accuracy by comparing mapped labels to true labels
accuracy <- sum(mapped_labels == true_labels) / length(true_labels)
print(paste("Clustering Accuracy:", round(accuracy * 100, 2), "%"))
```
\bigskip

# Question 3: Spectral Clustering [15 pts]

For this question, let's use the spectral clustering function `specc()` from the `kernlab` package. Let's also consider all pixels, instead of just the top 50 features. Specify your own choice of the kernel and the number of clusters. Report your results and compare them with the previous clustering methods.

```{r}
#install.packages("kernlab")
library(kernlab)

# Specify the number of clusters (for example, using the number of unique labels in the dataset)
num_clusters <- length(unique(mnist$Digit))

# Apply spectral clustering with an RBF kernel (Gaussian) and the specified number of clusters
spectral_clusters <- specc(as.matrix(mnist[, -1]), centers = num_clusters, kernel = "rbfdot")

# Extract the cluster assignments
cluster_labels <- as.factor(spectral_clusters@.Data)

# Compare with true labels
true_labels <- mnist$Digit
# Create a contingency table between true labels and cluster labels
table_result <- table(true_labels, cluster_labels)

# Map each cluster label to the true label with the highest count in each cluster
cluster_to_label <- apply(table_result, 2, function(x) names(which.max(x)))

# Replace cluster labels with mapped labels
mapped_labels <- cluster_to_label[as.character(cluster_labels)]

# Calculate accuracy
accuracy <- sum(mapped_labels == true_labels) / length(true_labels)
print(paste("Spectral Clustering Accuracy:", round(accuracy * 100, 2), "%"))

```
The accuracy is higher than all of the previous methods.
\bigskip
