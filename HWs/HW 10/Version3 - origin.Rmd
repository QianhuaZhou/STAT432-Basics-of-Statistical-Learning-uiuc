---
title: "Stat 432 Homework 10"
date: 'Assigned: Oct 28, 2024; <span style=''color:red''>Due: 11:59 PM CT, Nov 7,
  2024</span>'
output:
  pdf_document:
    toc: true
    toc_depth: 2
  html_document:
    df_print: paged
    toc: true
    toc_depth: '2'
  word_document:
    toc: true
    toc_depth: '2'
editor_options:
  chunk_output_type: console
---

<style>
  body {
  text-align: justify}
</style>

```{css, echo=FALSE}
  .solution {
  background-color: #CCDDFF; /* Replace with your desired color */
  }
  
  blockquote {
  background-color: #CCDDFF; /* Replace with your desired color */
  font-family: "Times New Roman", serif; /* Change to your desired font family */
  /* font-size: 16px; /* Change to your desired font size */
  font-weight: bold; /* Makes the text bold */
  }
```

# Question 1: K-means Clustering [65 pts]

In this question, we will code our own k-means clustering algorithm. The __key requirement__ is that you __cannot write your code directly__. You __must write a proper prompt__ to describe your intention for each of the function so that GPT (or whatever AI tools you are using) can understand your way of thinking clearly, and provide you with the correct code. We will use the handwritten digits dataset from HW9 (2600 observations). Recall that the k-means algorithm iterates between two steps:

  + Assign each observation to the cluster with the closest centroid.
  + Update the centroids to be the mean of the observations assigned to each cluster.
  
You do not need to split the data into train and test. We will use the whole dataset as the training data. Restrict the data to just the digits 2, 4 and 8. And then perform marginal variance screening to __reduce to the top 50__ features. After this, complete the following tasks. Please read all sub-questions a, b, and c before you start, and think about how different pieces of the code should be structured and what the inputs and outputs should be so that they can be integrated. For each question, you need to document your prompt to GPT (or whatever AI tools you are using) to generate the code. __You cannot wirte your own code or modify the code generated by the AI tool in any of the function definitions.__
```{r, eval=FALSE, include=TRUE}
  # inputs to download file
  fileLocation <- "https://pjreddie.com/media/files/mnist_train.csv"
  numRowsToDownload <- 2600
  localFileName <- paste0("mnist_first", numRowsToDownload, ".RData")
  
  # download the data and add column names
  mnist2600 <- read.csv(fileLocation, nrows = numRowsToDownload)
  numColsMnist <- dim(mnist2600)[2]
  colnames(mnist2600) <- c("Digit", paste("Pixel", seq(1:(numColsMnist - 1)), sep = ""))
  
  # save file
  # in the future we can read in from the local copy instead of having to redownload
  save(mnist2600, file = localFileName)
```
```{r load_mnist_data, echo = FALSE, results = 'hide'}
mnist2600 <- get(load("/Users/evelynzhou/Library/CloudStorage/OneDrive-UniversityofIllinois-Urbana/STAT 432/HW/HW 10/mnist_first2600.RData"))
```
```{r}
  # you can load the data with the following code
  #load(file = localFileName)
  dim(mnist2600)
  
  # Filter the dataset to include only specific digits (2, 4, and 8)
  mnist <- mnist2600[mnist2600$Digit %in% c(2, 4, 8), ]
  # Apply variance screening to reduce features to the top 50 by marginal variance
  variances <- apply(mnist[, -1], 2, var)
  top_features <- order(variances, decreasing = TRUE)[1:50]
  X <- mnist[, c(1, top_features + 1)]
  dim(X)
```

  a. [20 pts] In this question, we want to ask GPT to write a function called `cluster_mean_update()` that takes in three arguments, the data $X$, the number of clusters $K$, and the cluster assignments. And it outputs the updated centroids. Think about how you should describe the task to GPT (your specific requirements of how these arguments and the output should structured) so that it can understand your intention. You need to request the AI tool to provide sufficient comments for each step of the function. After this, test your function with the training data, $K = 3$ and a random cluster assignment. 
  
### Prompt

Implement a function in R called cluster_mean_update() to update the centroids of clusters as part of a K-means clustering algorithm. This function should be designed with efficiency in mind, utilizing vectorized operations where possible.

- Function Inputs:
  - X: A 2D matrix or dataframe where each row represents an observation and each column represents a feature.
  - K: An integer representing the number of clusters.
  - cluster_assignments: A vector of length equal to the number of rows in X. Each element is an integer from 1 to K, representing the assigned cluster for each observation.

- Function Outputs
  - centroids: A 2D matrix with K rows and a number of columns equal to the features in X. Each row i represents the mean (centroid) of the observations assigned to cluster i.

It should begin by initializing an empty matrix to store the centroids. For each cluster, it filters the observations in X that are assigned to that cluster based on cluster_assignments, then calculates the mean of each feature across these observations and stores the result in the corresponding row of the centroids matrix. If a cluster has no observations, the function sets that centroid to a row of zeros, which ensures that each row in the output matrix accurately represents the mean (centroid) of the points assigned to each cluster. Include comments to clearly explain each part of the function.

After implementing the cluster_mean_update(), test it using sample data with K = 3 and a sample cluster_assignments vector to validate correct operation.


```{r}
cluster_mean_update <- function(X, K, cluster_assignments) {
  # X: Data frame or matrix of observations (rows) and features (columns)
  # K: Number of clusters
  # cluster_assignments: Vector indicating the cluster assignment for each observation
  
  # Initialize a matrix to store the updated centroids
  centroids <- matrix(0, nrow = K, ncol = ncol(X))
  
  # Assign column names to the centroids matrix for clarity
  colnames(centroids) <- colnames(X)
  
  # Loop through each cluster to calculate the mean for each one
  for (k in 1:K) {
    # Subset the data for the current cluster
    cluster_data <- X[cluster_assignments == k, , drop = FALSE]
    
      # Check if the cluster has any assigned observations
  if (nrow(cluster_data) > 0) {
    # Calculate the mean of each feature for the cluster
    centroids[k, ] <- colMeans(cluster_data)
  } else {
    # If no observations are assigned to the cluster, set centroid to zeros
    centroids[k, ] <- 0
    cat(paste("Centroid", k, "has no assigned observations and is set to zeros.\n"))
  }
  }
  
  return(centroids)
}
```

```{r}
# Testing the cluster_mean_update Function
set.seed(123)  # Set seed for reproducibility
K <- 3
# Generate random cluster assignments
random_cluster_assignments <- sample(1:K, nrow(X), replace = TRUE)

# Get the updated centroids
centroids <- cluster_mean_update(X, K, random_cluster_assignments)
print(centroids)
```
\bigskip

  b. [20 pts] Next, we want to ask GPT to write a function called `cluster_assignments()` that takes in two arguments, the data $X$ and the centroids. And it outputs the cluster assignments. Think about how you should describe the task to GPT so that this function would be compatible with the previous function to achieve the k-means clustering. You need to request the AI tool to provide sufficient comments for each step of the function. After this, test your function with the training data and the centroids from the previous step.
  
### Prompt

Implement a function in R called cluster_assignments() to assign each observation in a dataset to the closest centroid in a K-means clustering algorithm. This function will be used alongside the previously defined cluster_mean_update() function.
The goal of cluster_assignments() is to assign each data point in a dataset X to the nearest cluster centroid based on Euclidean distance. 

- Function Inputs
  - X: A 2D matrix or dataframe where each row represents an observation and each column - represents a feature.
  - centroids: A 2D matrix where each row represents the centroid of a cluster. The matrix has K rows (one for each cluster) and the same number of columns as X.

- Function Outputs

The function should return a vector, assignments, with a length equal to the number of rows in X. Each element in this vector is an integer between 1 and K, representing the cluster assignment for each observation.

Include comments to clearly explain each part of the function, particularly the distance calculation and assignment process. Ensure the function is compatible with the cluster_mean_update() function for seamless integration and test the function using centroids generated by it.

After implementing the cluster_assignments() function, test it with the training data and the centroids from the previous step.
  
```{r}
cluster_assignments <- function(X, centroids) {
  # Ensure X and centroids are numeric matrices
  X <- as.matrix(X)
  centroids <- as.matrix(centroids)

  # Initialize a vector to store the cluster assignments
  assignments <- integer(nrow(X))
  
  # Iterate through each observation in X
  for (i in 1:nrow(X)) {
    # Calculate the Euclidean distance between the data point and each centroid
    distances <- apply(centroids, 1, function(centroid) sum((X[i, ] - centroid)^2))
    
    # Assign the data point to the closest centroid
    assignments[i] <- which.min(distances)
  }
  
  return(assignments)
}
```


```{r, message=FALSE, warning=FALSE}
# Testing the cluster_assignments Function
# Use the centroids from the previous test
assignments <- cluster_assignments(X[, -1], centroids)
print(assignments)
```

\bigskip

  c. [20 pts] Finally, we want to ask GPT to write a function called `kmeans()`. What arguments should you supply? And what outputs should be requested? Again, think about how you should describe the task to GPT. Test your function with the training data, $K = 3$, and the maximum number of iterations set to 20. For this code, you can skip the multiple starting points strategy. However, keep in mind that your solution maybe suboptimal.
  
### Prompt

Implement a function in R called kmeans() to perform K-means clustering on a dataset. This function will use both cluster_assignments() and cluster_mean_update() for seamless integration and efficient operation.

The kmeans() function orchestrates the K-means clustering algorithm by iteratively assigning observations to the nearest centroids and updating centroids until convergence or reaching a specified maximum number of iterations.

- Function Inputs
  - X: A 2D matrix or dataframe where each row represents an observation and each column represents a feature.
  - K: The number of clusters, an integer specifying the number of clusters to form.
max_iter: The maximum number of iterations for the algorithm (default set to 20).

- Function Outputs
  - centroids: A matrix of the final centroids after convergence or reaching the maximum iterations, with K rows (one for each cluster) and the same number of columns as X.
  - final_assignments: A vector of cluster assignments for each observation in X.
  - iterations: The number of iterations performed(can directly print the number of iterations).
  
Include comments to clearly explain each part of the function, particularly the distance calculation and assignment process.

After implementing the cluster_assignments() function, test it with the training data, $K = 3$, and the maximum number of iterations set to 20.

```{r}
kmeans <- function(X, K, max_iter = 20) {
  # Initialize centroids by selecting K random observations from X
  centroids <- X[sample(nrow(X), K), -1]
  
  # Initialize previous centroids to a matrix of zeros to ensure convergence check is met on the first iteration
  prev_centroids <- matrix(0, nrow = K, ncol = ncol(X) - 1)
  
  # Initialize iteration counter
  iter <- 1
  
  # Iterate until convergence or until the maximum number of iterations is reached
  while (iter <= max_iter && !all(centroids == prev_centroids)) {
    # Update cluster assignments using the cluster_assignments() function
    assignments <- cluster_assignments(X, centroids)
    
    # Update centroids using the cluster_mean_update() function
    prev_centroids <- centroids
    centroids <- cluster_mean_update(X, K, assignments)
    
    # Increment iteration counter
    iter <- iter + 1
  }
  
  # Return the final centroids, cluster assignments, and number of iterations performed
  return(list(final_centroids = centroids, final_assignments = assignments, iterations = iter))
}
```

```{r, message=FALSE, warning= FALSE}
# Testing the function
kmeans_result <- kmeans(X, K = 3, max_iter = 20)
print(kmeans_result$final_centroids)
print(kmeans_result$final_assignments)
cat("Total Iterations Performed:", kmeans_result$iterations)
```

\bigskip

  d. [5 pts] After completing the above tasks, check your clustering results with the true labels in the training dataset. Is your code working as expected? What is the accuracy of the clustering? You are not restricted to use the AI tool from now on. Comment on whether you think the code generated by GPT can be improved (in any ways).
```{r, message=FALSE, warning=FALSE}
# Extract true labels and predicted assignments
true_labels <- X$Digit
predicted_labels <- kmeans_result$final_assignments # from the kmeans function
# Create a mapping between clusters and true labels
# For each cluster, find the most common true label
library(dplyr)
cluster_mapping <- data.frame(true_label = true_labels, cluster = predicted_labels) %>%
  group_by(cluster) %>%
  summarise(predominant_label = as.numeric(names(which.max(table(true_label)))))
# Map predicted cluster assignments to the predominant true label
mapped_predictions <- sapply(predicted_labels, function(cluster) {
  cluster_mapping$predominant_label[cluster_mapping$cluster == cluster]
})
# Create the confusion matrix
confusion_matrix <- table(true_labels, predicted_labels)
print("Confusion Matrix:")
print(confusion_matrix)

# Calculate accuracy
accuracy <- mean(mapped_predictions == true_labels)
print(paste("Clustering Accuracy:", round(accuracy * 100, 2), "%"))
```
The code seems to be working as expected, as it correctly assigns observations to clusters and updates centroids based on the K-means algorithm. 
However, the code generated by GPT could be further improved in several ways:
First, running K-means multiple times with different random initializations can improve results, as K-means is sensitive to initial centroid positions. By selecting the clustering result with the lowest within-cluster variance (inertia) after multiple runs, we can often achieve better performance.

Applying dimensionality reduction techniques like Principal Component Analysis (PCA) can also help by reducing noise and focusing on the most informative features, which can make clustering more effective.

Another enhancement is to explore alternative clustering methods, such as Gaussian Mixture Models (GMM) or Spectral Clustering, which may better capture complex data structures.

Finally, we can improve efficiency by replacing loops with vectorized operations where possible. For example, in the cluster_assignments() function, using matrix operations to calculate distances between all observations and centroids at once would speed up the process, especially for larger datasets.

\bigskip

# Question 2: Hierarchical Clustering

In this question, we will use the hierarchical clustering algorithm to cluster the training data. We will use the same training data as in Question 1. Directly use the `hclust()` function in R to perform hierarchical clustering, but test different linkage methods (single, complete, and average) and euclidean distance. 

  a. [10 pts] Plot the three dendrograms and compare them. What do you observe? Which linkage method do you think is the most appropriate for this dataset?
```{r}
# Calculate the Euclidean distance matrix
dist_matrix <- dist(X, method = "euclidean")

# Perform hierarchical clustering with different linkage methods
hc_single <- hclust(dist_matrix, method = "single")
hc_complete <- hclust(dist_matrix, method = "complete")
hc_average <- hclust(dist_matrix, method = "average")

# Plot the dendrograms
par(mfrow = c(1, 3))  # Arrange plots side by side
plot(hc_single, main = "Single Linkage", xlab = "", sub = "", cex = 0.6)
plot(hc_complete, main = "Complete Linkage", xlab = "", sub = "", cex = 0.6)
plot(hc_average, main = "Average Linkage", xlab = "", sub = "", cex = 0.6)
par(mfrow = c(1, 1))  # Reset plot layout
```
Single Linkage:
- The dendrogram is highly elongated with many "chained" clusters, where individual data points are linked one-by-one.
- This chaining effect makes it harder to distinguish clear, well-separated clusters.
- Single linkage is generally sensitive to noise and outliers, which can result in this kind of chaining pattern.

Complete Linkage:
- The dendrogram is more balanced and compact, with clusters that appear well-separated.
- This linkage method tends to produce more spherical clusters, which is often desirable if the data has distinct groups.
- Complete linkage minimizes the maximum distance between clusters, making it less sensitive to outliers and better suited for identifying well-defined clusters.

Average Linkage:
- The average linkage dendrogram is somewhat between single and complete linkage in terms of cluster compactness.
- Clusters are generally balanced, though not as tight as with complete linkage.
- Clusters in average linkage merge at moderate heights, capturing some structure in the data but without achieving as clear separation between groups.

Conclusion:
Based on the structure observed in the dendrograms, **complete linkage** seems to be the most appropriate for this dataset. It provides well-separated, compact clusters that are less prone to the chaining effect seen in single linkage, making it easier to interpret the clustering structure.

\bigskip
  b. [10 pts] Choose your linkage method, cut the dendrogram to obtain 3 clusters and compare the clustering results with the true labels in the training dataset. What is the accuracy of the clustering? Comment on its performance. 
```{r}
# Perform complete linkage clustering on the distance matrix
hc_complete <- hclust(dist_matrix, method = "complete")

# Cut the dendrogram to obtain 3 clusters
clusters <- cutree(hc_complete, k = 3)

# Create a contingency table between true labels and predicted cluster labels
table_result <- table(true_labels, clusters)

# Map each cluster label to the most frequent true label within that cluster
cluster_to_label <- sapply(1:ncol(table_result), function(col) {
  true_label <- names(which.max(table_result[, col]))
  return(true_label)
})

# Convert the mapping result to a named character vector
names(cluster_to_label) <- colnames(table_result)

# Replace cluster labels with the mapped labels
mapped_labels <- as.character(cluster_to_label[as.character(clusters)])

confusion_matrix <- table(true_labels,clusters)
print("Confusion Matrix:")
print(confusion_matrix)

# Calculate accuracy by comparing mapped labels to true labels
accuracy <- sum(mapped_labels == true_labels) / length(true_labels)
print(paste("Clustering Accuracy:", round(accuracy * 100, 2), "%"))
```
The clustering accuracy achieved was 76.2%, indicating that the hierarchical clustering with complete linkage correctly grouped approximately 76% of observations based on the predominant label in each of the three clusters. This result reflects a moderate success in clustering similar digits together, though a considerable portion of observations were misclassified.

This level of accuracy is expected for an unsupervised clustering approach, as hierarchical clustering operates without knowledge of the true labels, relying solely on Euclidean distances between data points. Consequently, it may overlook subtle differences between digit images. The Euclidean distance metric can struggle to capture the complex structure in high-dimensional data like digit images, especially after reducing the dataset to the 50 most variable pixels. In high-dimensional spaces, Euclidean distance can lose its discriminatory power, making it challenging to distinguish clusters effectively.

Moreover, while complete linkage generally yields compact and well-separated clusters, it may not perfectly match the inherent structure of handwritten digits. Digits like 2, 4, and 8, which may share similar visual features in specific regions, can end up in mixed clusters, highlighting the limitations of distance-based clustering in capturing the nuanced variability within handwritten digit images.

\bigskip

# Question 3: Spectral Clustering [15 pts]

For this question, let's use the spectral clustering function `specc()` from the `kernlab` package. Let's also consider all pixels, instead of just the top 50 features. Specify your own choice of the kernel and the number of clusters. Report your results and compare them with the previous clustering methods.

```{r}
#install.packages("kernlab")
library(kernlab)

# Specify the number of clusters (for example, using the number of unique labels in the dataset)
num_clusters <- length(unique(mnist$Digit))

# Apply spectral clustering with an RBF kernel (Gaussian) and the specified number of clusters
spectral_clusters <- specc(as.matrix(mnist[, -1]), centers = num_clusters, kernel = "rbfdot")

# Extract the cluster assignments
cluster_labels <- as.factor(spectral_clusters@.Data)

# Compare with true labels
true_labels <- mnist$Digit
# Create a contingency table between true labels and cluster labels
table_result <- table(true_labels, cluster_labels)

# Map each cluster label to the true label with the highest count in each cluster
cluster_to_label <- apply(table_result, 2, function(x) names(which.max(x)))

# Replace cluster labels with mapped labels
mapped_labels <- cluster_to_label[as.character(cluster_labels)]

# Create a confusion matrix
confusion_matrix_spectral <- table(true_labels, spectral_clusters)
print(confusion_matrix_spectral)

# Calculate accuracy
accuracy <- sum(mapped_labels == true_labels) / length(true_labels)
print(paste("Spectral Clustering Accuracy:", round(accuracy * 100, 2), "%"))

```
The accuracy is 87.06 %.

We use the Radial Basis Function (RBF) kernel here, as it’s typically well-suited for image data by capturing similarity based on pixel intensity. We set the number of clusters to 3 to be consistent with the previous analyses. The spectral clustering output shows an accuracy of 87.06%, which is an improvement over the hierarchical clustering accuracy of 76.2% from Question 2. It suggests that spectral clustering was able to capture the structure of the digit data more effectively.

This is because spectral clustering often performs well on datasets where clusters are not linearly separable. Spectral clustering is capable of finding non-linear boundaries between clusters, which is a key advantage over hierarchical clustering. This flexibility helps it perform better on data where classes are not well-separated in a simple Euclidean sense, such as digit images. Spectral clustering uses a similarity graph (constructed from the RBF kernel) to represent the data. This graph-based approach can capture relationships between points that are not solely based on pairwise distances, providing a more nuanced view of clusters. In contrast, hierarchical clustering with complete linkage relies directly on Euclidean distances, which may not fully capture the relationships in high-dimensional image data.

The accuracy improvement (from 76.2% with hierarchical clustering to 87.06% with spectral clustering) indicates that spectral clustering is better suited for the MNIST data in this context. The non-linear similarities captured by the RBF kernel likely align more closely with the inherent structure of the digit images, leading to more accurate clusters.
