---
title: "Numerical Optimization: Basic Concepts"
author: "Ruoqing Zhu"
date: "Last Updated: `r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    code_folding: hide
    df_print: paged
    toc: yes
    toc_float:
      collapsed: true
      smooth_scroll: true
    toc_depth: '2'
  pdf_document:
    toc: yes
    toc_depth: 2
---

<style>
body {
text-align: justify}
</style>

```{r echo=FALSE, cache=FALSE}
  options(width = 1000)
  knitr::opts_chunk$set(fig.width=9, fig.height=7, out.width = "75%", fig.align = 'center')
  knitr::opts_chunk$set(class.source = "fold-show")
  knitr::opts_chunk$set(collapse=TRUE)
```

\def\bx{\mathbf{x}}
\def\bbeta{\boldsymbol \beta}
\def\by{\mathbf{y}}
\def\bH{\mathbf{H}}
\def\T{\text{T}}

## A Motivating Example: Linear Regression

Although we have the analytic solution of a linear regression, it can still be treated as an optimization problem. In addition, this view of a linear regression will benefit us when we introduce the Lasso and Ridge regressions. 

Let's consider a simple linear regression, in which we observe a set of observations $\{x_i, y_i\}_{i = 1}^n$, our optimization problem is to find parameters $\beta_0$ and $\beta_1$ to minimize the objection function, i.e., residual sum of squares (RSS):

\begin{align}
\underset{\bbeta}{\text{minimize}} \quad \ell(\bbeta) = \frac{1}{n} \sum_i (y_i - \beta_0 - x_i\beta_1)^2 \\
\end{align}

Let's first generate a set of data. We have two parameters, an intercept $\beta_= 0.5$ and a slope $\beta_1 = 1$. 

```{r}
  # generate data from a simple linear model 
  set.seed(20)
  n = 150
  x <- rnorm(n)
  y <- 0.5 + x + rnorm(n)
```

Suppose we have a candidate estimation of $\bbeta$, say $\widehat{\bbeta} = c(0.3, 1.5)$, then we can calculate the RSS using 

\begin{align}
\frac{1}{n} \sum_i \left( y_i - 0.3 - 1.5 \times x_i \right)^2 \\
\end{align}

We can then make a function that calculates the RSS for any value of $\bbeta$:

```{r}
  # calculate the residual sum of squares for a grid of beta values
  rss <- function(b, trainx, trainy) mean((trainy - b[1] - trainx * b[2])^2)

  # Try it on a beta value
  rss(b = c(0.3, 1.5), trainx = x, trainy = y)
```

Doing this on all such $\bbeta$ values would allow us to create a surface of the RSS, as a function of the parameters. The following R code uses the `plotly` package to create a 3D plot to more intuitively understand the problem. 

```{r}
  # create a grid of beta values and the corresponding RSS
  b0 <- b1 <- seq(0, 2, length = 20)
  z = matrix(apply(expand.grid(b0, b1), 1, rss, x, y), 20, 20)
  
  onepoint = data.frame("x" = 0.3, "y" = 1.5, "z" = rss(c(0.3, 1.5), x, y))
  
  # 3d plot of RSS using `plotly`
  library(plotly)
  plot_ly(x = b0, y = b1) %>%
      layout(plot_bgcolor='rgb(254, 247, 234)') %>% 
      layout(paper_bgcolor='transparent') %>% 
      add_surface(z = t(z), 
                  colorscale = 'Viridis') %>% 
      layout(scene = list(xaxis = list(title = "beta0"), 
             yaxis = list(title = "beta1"),
             zaxis = list(title = "RSS"))) %>% 
      add_markers(data = onepoint, 
                x = ~x, y = ~y, z = ~z, 
                marker = list(size = 6, color = "red", symbol = 104))
```

As we can see, the pre-specified point $(0.3, 1.5)$ is not at the bottom of this curve. For this optimization problem, our goal is to minimize the RSS. Hence, we would like to know what are the corresponding $\bbeta$ values. Numerical optimization is a research field that investigates such problems and their properties. 

## The `optim()` function

As a starting point, let's introduce the `optim()` function, which can be used to solve such problems. By default, its solves a minimization problem. 

```{r}
    # The solution can be solved by any optimization algorithm 
    lm.optim <- optim(par = c(2, 2), fn = rss, trainx = x, trainy = y)
```

  * The `par` argument specifies an initial value. In this case, it is $\beta_0 = \beta_1 = 2$. 
  * The `fn` argument specifies the name of a function (`rss` in this case) that can calculate the objective function. This function may have multiple arguments. However, the first argument has to be the parameter(s) that is being optimized. In addition, the parameters need to be supplied to the function as a vector, but not matrix, list or other formats. 
  * The arguments `trainx = x`, `trainy = y` specifies any additional arguments that the objective function `fn` (`rss`) needs. It behaves the same as if you are supplying this to the function `rss` it self.

```{r}
  lm.optim
```

The result shows that the estimated parameters (`$par`) are 0.567 and 1.017, with a functional value 1.077. The convergence code is 0, meaning that the algorithm converged. The parameter estimates are almost the same as `lm()`. 

```{r}
  # The solution form lm()
  summary(lm(y ~ x))$coefficients
```

## Basic Principles 

We will first introduce several properties that would help us to determine if a point $\bx$ attains the optimum. Then some examples will follow. These properties are usually applied to unconstrained optimization problems. They are essentially just describing the landscape around a point $\bx^\ast$ such that it becomes the local optimizer. Before we proceed, here are some things to be aware of:

  * All the following statements are __multidimensional__ since we may have more than one parameter to optimize. Hence, partial derivatives $\nabla$ are always vectors and second derivatives $\nabla^2$ are matrices, although some examples could be one-dimensional.
  * Instead of using $\bbeta$ as the argument of the function, we will use $\bx$, which is a $p$-dimensional vector. This may seem conflicting with our other lecture notes. However, this notation is consistent with the literature on this topic.   
  * To grasp the idea, we could assume that the functions are smooth, e.g., continuously differentiable. This would give us nice properties to help the analysis. Later on, we may see other examples that are not differentiable, such as the Lasso. But we will have other tools to deal with it.
  * We will also assume that we are dealing with a minimization problem of $f(\bx)$, while for maximization of $f(\bx)$, we can always consider minimizing $-f(\bx)$. Hence they are essentially the same. 

## First-order Property

__First-Order Necessary Condition__: 

> If $f$ is continuously differentiable in an open neighborhood of local minimum $\bx^\ast$, then $\nabla f(\bx^\ast) = \mathbf{0}$. 

To see why a local optimum must satisfy this property, it might be intuitive to see the following plot, which is generated using functions 

\begin{align}
\text{Left:} \qquad f_1(x) &= x^2 \\
\text{Right:} \qquad f_2(x) &= x^4 + 2\times x^3 - 5 \times x^2
\end{align}

It is easy to see that the minimizer of the left function is 0, while the minimizer of the right one is around -2.5, but there are also other points that are interesting. 

```{r echo = FALSE, fig.dim = c(12, 5), out.width = "75%", fig.align = 'center'}
  par(mfrow=c(1,2))
  par(mar=c(2,2,2,2))
  
  # a convex case 
  x = seq(-2, 2, 0.01)
  plot(x, x^2, type = "l", col = "deepskyblue", lwd = 1.5)
  points(0, 0, col = "red", pch = 19, cex = 2.5)
  
  # non-convex case
  x = seq(-4, 2, 0.01)
  plot(x, x^4 + 2*x^3 - 5*x^2, type = "l", col = "deepskyblue", lwd = 1.5)
  points(-2.5, (-2.5)^4 + 2*(-2.5)^3 - 5 *(-2.5)^2, col = "red", pch = 19, cex = 2.5)
  points(0, 0, col = "darkorange", pch = 19, cex = 1.5)
  points(1, -2, col = "red", pch = 19, cex = 1.5)
```

On the left hand side, we have a __convex function__, which looks like a bowl. The intuition is that, if $f(\bx)$ is a function that is smooth enough, and $\bx$ is a point with $\nabla f(\bx^\ast) \neq 0$, then by the Taylor expansion, we have, for any new point $\bx^{new}$ in the neighborhood of $\bx$, we can approximate its function value 

$$f(\bx^\text{new}) \approx f(\bx) + \nabla f(\bx) (\bx^\text{new} - \bx)$$
Regardless of whether $\nabla f(\bx)$ is positive or negative, we can always find a new point $\bx^\text{new}$ that makes $\nabla f(\bx) (\bx^\text{new} - \bx)$ smaller than 0. Hence, this new point would have a smaller functional value than $\bx$. 

Let's apply this result to the two problems above. Note that this would not be possible with more complex functions. By taking derivatives we have 

\begin{align}
\text{Left:} \qquad \nabla f_1(x) &= 2x \\
\text{Right:} \qquad \nabla f_2(x) &= 4x^3 + 6 x^2 - 10x = 2x (x - 1) (2 x + 5)\\
\end{align}

Hence, for the left one, the only point that makes the derivative 0 is $x = 0$, which is indeed what we see on the figure. For the right one, there are three $x$ values that would make this 0: $x =0, 1$ and $-2.5$. Are they all minimizers of this function? Of course not, however, for different reasons. 

  * $x = 0$ is a maximizer rather than a minimizer. Since we only checked if the slope if "flat" but didn't care if its facing upward or downward, our condition cannot tell the difference. That's why $\nabla f(\bx^\ast) = \mathbf{0}$ is __only a necessary condition, but not a sufficient condition__. 
  * $x = 1$ is in fact a minimizer, although it is __not a global minimizer, but only a local minimizer__. In this course, we will see some examples of this such as the $k$-means clustering algorithm, which could lead to a local minimizer. Although we will not focus on this issue, there are two things to consider: 
      * When the problem we are trying to solve is a __convex function__ (also needs to be in a convex domain), a local minimum is guaranteed to be a global minimum. This is the case of $f_1(x)$. 
      * For a non-convex problem, numerical approaches (instead of analytic solution) can start from different initial points (see our example of `optim()`) and this may lead to different solutions, some are local, some can be global. Hence, try different initial points, and compare their end results can give us a better chance of getting a global solution. 

### Practice question

Suppose $f(x) = \exp(-x) + x$, what is the minimizer of this function? Produce a figure to visualize the result.
  
```{r class.source = NULL, eval = FALSE}
  x = seq(-2, 2, 0.01)
  plot(x, exp(-x) + x, type = "l", lwd = 2)
```

## Second-order Property

__Second-order Sufficient Condition__: 

> $f$ is twice continuously differentiable in an open neighborhood of $\bx^\ast$. If $\nabla f(\bx^\ast) = \mathbf{0}$ and $\nabla^2 f(\bx^\ast)$ is positive definite, i.e.,
$$
\nabla^2 f(\bx) = \left(\frac{\partial^2 f(\bx)}{\partial x_i \partial x_j}\right) = \bH(\bx) \succeq 0
$$
then $\bx^\ast$ is a strict local minimizer of $f$. 

Here $\bH(\bx)$ is called the __Hessian matrix__, which will be frequently used in second-order methods. We can easily check this property for our examples: 

\begin{align}
\text{Left:} \qquad \nabla^2 f_1(x) &= 2 \\
\text{Right:} \qquad \nabla^2 f_2(x) &= 12x^2 + 12 x - 10\\
\end{align}

Hence for $f_1$, the Hessian is positive definite, and the solution is a minimizer. While for $f_2$, $\nabla^2 f_2(-2.5) = 35$, $\nabla^2 f_2(0) = -10$ and $\nabla^2 f_2(1) = 14$. This implies that $-2.5$ and $1$ are local minimizers and $0$ is a local maximizer. These conditions are sufficient, but again, they only discuss local properties, not global properties. 

## Algorithm 

Most optimization algorithms follow the same idea: starting from a point $\bx^{(0)}$ (which is usually specified by the user) and move to a new point $\bx^{(1)}$ that improves the objective function value. Repeatedly performing this to get a sequence of points $\bx^{(0)}, \bx^{(1)}, \bx^{(2)}, \bx^{(3)}, \ldots$ until the certain stopping criterion is reached. 

We have introduced two properties that provide a candidate of a new target point: second-order methods (Newton's method) and first-order methods (gradient descent). Repeatedly applying these strategies in an iterative algorithm would allow us to find a good __optimizer__ after a good number of iterations. Of course, this iterative algorithm has to stop at certain point. Before showing examples, we mention that a __stopping criterion__ could be 

  * Using the gradient of the objective function: $\lVert \nabla f(\bx^{(k)}) \rVert < \epsilon$
  * Using the (relative) change of distance: $\lVert \bx^{(k)} - \bx^{(k-1)} \rVert / \lVert \bx^{(k-1)}\rVert< \epsilon$ or $\lVert \bx^{(k)} - \bx^{(k-1)} \rVert < \epsilon$
  * Using the (relative) change of functional value: $| f(\bx^{(k)}) - f(\bx^{(k-1)})| < \epsilon$ or $| f(\bx^{(k)}) - f(\bx^{(k-1)})| / |f(\bx^{(k)})| < \epsilon$
  * Stop at a pre-specified number of iterations.

Most algorithms differ in terms of how to move from the current point $\bx^{(k)}$ to the next, better target point $\bx^{(k+1)}$. This may depend on the smoothness or structure of $f$, constrains on the domain, computational complexity, memory limitation, and many others. 

## The `optim()` function, again

We can use the `optim()` function to help us. Let's first define the functions. 

```{r}
  f1 <- function(x) x^2
  f2 <- function(x) x^4 + 2*x^3 - 5*x^2
```

We will use a method called `BFGS`, which a very popular approach and be introduced later. For $f_1(x)$, the algorithm finds the minimizer in just one step, while for the second problem, its depends on the initial point. 

```{r}
  # First problem
  optim(par = 3, fn = f1, method = "BFGS")
  
  # Second problem, ends with local minimizer 1
  optim(par = 10, fn = f2, method = "BFGS")
  
  # Try a new starting value, ends with global minimizer -2.5
  optim(par = 3, fn = f2, method = "BFGS")  
```

### Practice question

Going back to the example $f(x) = \exp(-x) + x$. Write the corresponding objective function and use the `optim()` function to solve it. 
  
```{r class.source = NULL, eval = FALSE}
  f3 <- function(b) exp(-b) + b

  optim(par = 3, fn = f3, method = "BFGS")
```