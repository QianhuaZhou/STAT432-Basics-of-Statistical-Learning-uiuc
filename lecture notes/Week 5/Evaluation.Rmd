---
title: "Classification Model Evaluation"
author: "Ruoqing Zhu"
date: "Last Updated: `r format(Sys.time(), '%B %d, %Y')`"
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    code_folding: hide
    df_print: paged
    toc: yes
    toc_float:
      collapsed: true
      smooth_scroll: true
    toc_depth: '2'
header-includes:
  - \def\cD{\cal{D}}
  - \def\bA{\mathbf{A}}
  - \def\bB{\mathbf{B}}
  - \def\bX{\mathbf{X}}
  - \def\bH{\mathbf{H}}
  - \def\bI{\mathbf{I}}
  - \def\bU{\mathbf{U}}
  - \def\bD{\mathbf{D}}
  - \def\bV{\mathbf{V}}
  - \def\bS{\mathbf{S}}
  - \def\bx{\mathbf{x}}
  - \def\by{\mathbf{y}}
  - \def\bs{\mathbf{s}}
  - \def\br{\mathbf{r}}
  - \def\bu{\mathbf{u}}
  - \def\be{\mathbf{e}}
  - \def\bv{\mathbf{v}}
  - \def\bp{\mathbf{p}}
  - \def\bzero{\mathbf{0}}
  - \def\bbeta{\boldsymbol \beta}
  - \def\bmu{\boldsymbol \mu}
  - \def\bepsilon{\boldsymbol \epsilon}
  - \def\T{\text{T}}
  - \def\Trace{\text{Trace}}
  - \def\Cov{\text{Cov}}
  - \def\Var{\text{Var}}
  - \def\E{\text{E}}
  - \def\pr{\text{pr}}
  - \DeclareMathOperator*{\argmin}{arg\,min}
  - \DeclareMathOperator*{\argmax}{arg\,max}
---

<style>
body {
text-align: justify}
</style>

```{r set-options, echo=FALSE, cache=FALSE}
  options(width = 1000)
  knitr::opts_chunk$set(fig.width=6, fig.height=6, out.width = "45%", fig.align = 'center')
  knitr::opts_chunk$set(class.source = "fold-show")
  knitr::opts_chunk$set(collapse=TRUE)
```



## Evaluating Classification Models

In previous lectures we always use the classification error as a criterion to evaluate a classification error. That is 

$$\frac{1}{n_\text{test}} \sum_{i\in \text{test}} \mathbf{1}\{\widehat{y}_i \neq y_i\} $$
For most applications, this might be OK. For example, we can predict $\widehat{y}$ as 1 if the estimated probability form a logistic regression $\widehat{P}(Y | X = x)$ is larger than 0.5. However, if we have an __unbalanced data__ situation, this may not provide enough we might be in trouble, since most or even all observations will be predicted as one class. And the prediction accuracy would simply be the proportion of $y_i$'s being one of the class. For example, if we have a dataset that 90% of the observations are label 0, then simply predicting all observations to 0 will give us 90% accuracy. 

For logistic regression, or any classification models, this situation may happen when the dataset is extremely unbalanced. Here is an example from the `ROSE` package.

```{r}
  library(ROSE)

  # this is a simulated data
  data(hacide)

  # class label frequency in testing data
  table(hacide.test$cls)
  
  # fit a logistic regression
  logistic.fit <- glm(cls ~., data = hacide.train, family = binomial)
  pred = predict(logistic.fit, newdata = hacide.test, type = "response")
```

If we use 0.5 as the cut-off, then everything is predicted into 0. However, the error rate of this mode is $5 / (245 + 5)$ = 0.02. 

```{r}
  table(pred > 0.5, hacide.test$cls)
```

However, if we use 0.11 as the cut-off, we can identify a subset of observations with high chance of being 1. Although the error rate is worse $7 / 250 = 0.028$, but this model is more useful. But how do we evaluate a model?  

```{r}
  table(pred > 0.11, hacide.test$cls)
```

It seems that we 1) need to use different cut-off values, which will provide a more comprehensive evaluation of the model performance, and 2) need some new measurements other than the prediction error to evaluate different results generated by these different cut-offs. 

## Example: Diagnostic Testing

The approaches we are considering are originated from diagnostic testing. For example, the anitgene test of SARS-CoV-2 works by binding antibody with the virus. A user can then visualize whether there are captured virus on the device. Preganency test works in the same way. 

<center>


add covid.png here

</center>

However, these tests are not 100% accurate. For example, one need to (visually) decide if the test line on the device is strong enough to conclude a positive test result. If this line is too weak, then one may make a wrong decision. 

```{r}
  table(pred > 0.01, hacide.test$cls)
```

Depending on the situation, there could be a benefit of using higher error rate, but being more sensitive of the test. For example, if we use 0.01 as the cut-off, we can detect 4 out of 5 positive samples. However, to do this, we need to report 58 samples as positive. This is not a very specific test. If we use 0.11 as the cut-off, we report 8 positives, with 3 of them as correct ones. All of these decisions are actually worse than the 5/250 error rate by simply reporting everything as negative. However, that model is not really useful. 

## Sensitivity and Specificity

$\quad$       | True 0 | True 1 
-----: | :----: | :----: 
__Predict as 0__ | True Negative (TN) | False Negative (FN)
__Predict as 1__ | False Positive (FP) | True Positive (TP)

  * __Sensitivity__ (also called “Recall”) is the defined as the __true positive rate__ (among the True 1 population, what proportion is correctly identified by the model)
  $$\text{Sensitivity} = \frac{\text{TP}}{\text{TP} + \text{FN}}$$
  * __Specificity__ is the defined as the __true negative rate__ (among the True 0 population, what proportion is correctly identified by the model)
  $$\text{Specificity} = \frac{\text{TN}}{\text{TN} + \text{FP}}$$

In the previous example, if we use 0.5 as the cut-off, the sensitivity is $0 / 5 = 0$, meaning that the model is not sensitive in terms of telling which subject may be at risk of an event. But the model is super specific (100%) because it is very conservative in terms of concluding any one at risk. 

On the other hand, if we use 0.11 as the cut-off, the sensitivity is much better $3/5 = 60%$, but we do not sacrifice much the specificity, $240 / 245 = 97.96%$. How about we summarize all of these information into one measurement? 

## The ROC Curve and AUC

If we alter the cut off value, we can get different sensitivity and specificity values for each choice of the cut off value. Then we can plot `1 - specificity` (false positive rate) versus the `sensitivity` (true positive rate). This is called the ROC (Receiver Operating Characteristic) curve, and it can be calculated automatically with existing packages. The closer this curve to the top-left, the better performance this model is. A common measure is the area under the ROC curve (AUC). 

```{r}
  library(ROCR)
  roc <- prediction(pred, hacide.test$cls)
  
  # calculates the ROC curve
  perf <- performance(roc,"tpr","fpr")
  plot(perf,colorize=TRUE)
  
  # this computes the area under the curve
  performance(roc, measure = "auc")@y.values[[1]]
```

There are many other packages that can perform similar calculations. However, be careful that some of them may require a specific orientation of your confusion table. Always read the documentation before using a new package. 

### Practice Question

Use the following data to calculate the AUC if we fit a logistic regression using the `glm()` function. 

```{r}
    set.seed(1)    
    n = 150
    x = rnorm(n)
    eta = exp(-1 + 2*x) / (1 + exp(-1 + 2*x))
    y = rbinom(n, size = 1, prob = eta)
```

```{r class.source = NULL, eval = FALSE}
    logistic.fit <- glm(y ~ x, family = binomial)
    pred = predict(logistic.fit, newdata = data.frame("x" = x), type = "response")
    
    roc <- prediction(pred, y)
    perf <- performance(roc,"tpr","fpr")
    plot(perf,colorize=TRUE)
    performance(roc, measure = "auc")@y.values[[1]]
```

## Cross-validation Using AUC

We can use AUC as the criteria to select the best model instead of using the prediction accuracy. In the `glmnet` package, we can specify `type.measure = "auc"`. 

```{r}
    library(glmnet)
    library(ElemStatLearn)
    data(SAheart)
    lasso.fit = cv.glmnet(x = data.matrix(SAheart[, 1:9]), y = SAheart[,10],
                          nfold = 10, family = "binomial", type.measure = "auc")
  
    plot(lasso.fit)
    lasso.fit$lambda.min
    coef(lasso.fit, s = "lambda.min")
```

For the AUC, the larger value the better. Hence we pick the $\lambda$ value that gives the best cross-validated AUC. This is still called `lambda.min`. 

### Practice Question

Use the Cleveland heart disease data and use `num > 0` as the event definition. Fit a ridge regression and use AUC to select the best model. 

```{r}
    heart = read.csv("processed_cleveland.csv")
    heart$Y = as.factor(heart$num > 0)
    table(heart$Y)
```

```{r class.source = NULL, eval = FALSE}
    lasso.fit = cv.glmnet(x = data.matrix(heart[, 1:13]), 
                          y = heart[,15], nfold = 10, 
                          family = "binomial", 
                          alpha = 1,
                          type.measure = "auc")
  
    plot(lasso.fit)
    lasso.fit$lambda.min
    coef(lasso.fit, s = "lambda.min")
```

