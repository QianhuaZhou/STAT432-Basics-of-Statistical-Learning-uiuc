"0","# Step 1: Split the data into training and testing sets"
"0","set.seed(123)"
"0","train_index <- createDataPartition(data_for_model$price, p = 0.5, list = FALSE)"
"0","train_data <- data_for_model[train_index, ]"
"0","test_data <- data_for_model[-train_index, ]"
"0",""
"0","# Step 2: Normalize the predictor variables (k-NN requires normalization)"
"0","normalize <- function(x) {"
"0","  return((x - min(x)) / (max(x) - min(x)))"
"0","}"
"0",""
"0","train_data_normalized <- train_data"
"0","test_data_normalized <- test_data"
"0",""
"0","train_data_normalized[, c(""PC1"", ""PC2"", ""cluster"")] <- lapply("
"0","  train_data_normalized[, c(""PC1"", ""PC2"", ""cluster"")], "
"0","  normalize"
"0",")"
"0","test_data_normalized[, c(""PC1"", ""PC2"", ""cluster"")] <- lapply("
"0","  test_data_normalized[, c(""PC1"", ""PC2"", ""cluster"")], "
"0","  normalize"
"0",")"
"0",""
"0","# Step 3: Extract predictors and target variable"
"0","train_x <- train_data_normalized[, c(""PC1"", ""PC2"", ""cluster"")]"
"0","test_x <- test_data_normalized[, c(""PC1"", ""PC2"", ""cluster"")]"
"0","train_y <- train_data$price"
"0","test_y <- test_data$price"
"0",""
"0","# Step 4: Tune k-NN model using cross-validation"
"0","set.seed(123)"
"0","tune_results <- train("
"0","  x = train_x,"
"0","  y = train_y,"
"0","  method = ""knn"","
"0","  trControl = trainControl(method = ""cv"", number = 5),  # 5-fold cross-validation"
"0","  tuneGrid = data.frame(k = seq(3, 20, by = 2)) "
"0",")"
"0",""
"0","best_k <- tune_results$bestTune$k "
"0",""
"0","# Step 5: Use the best k to predict on the test set"
"0","predictions <- knn("
"0","  train = train_x,"
"0","  test = test_x,"
"0","  cl = train_y,"
"0","  k = best_k"
"0",")"
"0",""
"0","# Step 6: Evaluate performance"
"0","predictions <- as.numeric(predictions)"
"0",""
"0","mean_test_price <- mean(test_y)"
"0","normalized_mse <- mean((test_y - predictions)^2) / mean_test_price^2"
"0",""
"0","cat(""Best k:"", best_k, ""\n"")"
"1","Best k:"
"1"," "
"1","19"
"1"," "
"1","
"
"0","cat(""Normalized MSE:"", normalized_mse, ""\n"")"
"1","Normalized MSE:"
"1"," "
"1","2.951454"
"1"," "
"1","
"
"0","# Step 7: Visualize k-NN Regression with Different k-values"
"0","x_seq <- seq(min(train_data_normalized$PC1), max(train_data_normalized$PC1), length.out = 100)"
"0",""
"0","knn_fit <- function(k, train_x, train_y, test_x) {"
"0","  knn_pred <- knn.reg(train = as.matrix(train_x), test = as.matrix(test_x), y = train_y, k = k)"
"0","  return(knn_pred$pred)"
"0","}"
"0",""
"0","# Create multiple k values to visualize"
"0","k_values <- c(1, 5, 10, 19, 50)"
"0","plots <- list()"
"0",""
"0","for (k in k_values) {"
"0","  knn_predictions <- knn_fit(k, train_x = train_data_normalized$PC1, train_y = train_y, test_x = x_seq)"
"0","  plot_data <- data.frame("
"0","    x_seq = x_seq,         "
"0","    knn_fit = knn_predictions"
"0","  )"
"0",""
"0","  p <- ggplot() +"
"0","    geom_point(data = train_data_normalized, aes(x = PC1, y = train_y), alpha = 0.5, size = 2) +  "
"0","    geom_line(data = plot_data, aes(x = x_seq, y = knn_fit), color = ""orange"", size = 1.2) + "
"0","    labs(title = paste(""k ="", k, ""k=19 optimal""), x = ""PC1"", y = ""Price"") +"
"0","    theme_minimal()"
"0",""
"0","  plots[[paste0(""k_"", k)]] <- p"
"0","}"
"0","do.call(grid.arrange, c(plots, ncol = 3))"
