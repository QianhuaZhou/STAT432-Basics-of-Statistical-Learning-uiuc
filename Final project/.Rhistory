str_detect(data$ext_col, "(?i)white|snow|pearl|alpine|glacier|frozen|chalk|beige|silver|grey|gray") ~ "Light",
str_detect(data$ext_col, "(?i)red|ruby|scarlet|garnet|firecracker|violet|flame|vermilion|pink|yellow|gold|orange|green|blue|blu|turquoise|aqua|lapis|denim|azure") ~ "Vivid",
is.na(data$ext_col) | data$ext_col == "" ~ "Unknown",
TRUE ~ "Other"
)
ext_col_summary <- table(data$ext_col, useNA = "ifany")
print(ext_col_summary)
data$ext_col[data$int_col == ""] <- NA
sum(is.na(data$int_col))
data$int_col <- case_when(
str_detect(data$int_col, "(?i)black|noir|onyx|obsidian|carbon|brown|bronze|espresso|dark") ~ "Dark",
str_detect(data$int_col, "(?i)white|snow|pearl|alpine|glacier|frozen|chalk|beige|silver|grey|gray") ~ "Light",
str_detect(data$int_col, "(?i)red|ruby|scarlet|garnet|firecracker|violet|flame|vermilion|pink|yellow|gold|orange|green|blue|turquoise|aqua|lapis|denim|azure") ~ "Vivid",
is.na(data$int_col) | data$int_col == "" | data$int_col == "-" ~ "Unknown",
TRUE ~ "Other"
)
int_col_summary <- table(data$int_col, useNA = "ifany")
print(int_col_summary)
data$ext_col[data$accident == ""] <- NA
sum(is.na(data$accident))
data$accident <- ifelse(data$accident == "None reported", 0,
ifelse(data$accident == "At least 1 accident or damage reported", 1, -1))
data$accident[is.na(data$accident)] <- 0
accident_summary <- table(data$accident, useNA = "ifany")
print(accident_summary)
clean_title_summary <- table(data$clean_title, useNA = "ifany")
print(clean_title_summary)
data$clean_title <- ifelse(data$clean_title == "Yes", 1, 0)
data$clean_title[is.na(data$clean_title)] <- 0
clean_title_summary <- table(data$clean_title, useNA = "ifany")
print(clean_title_summary)
data$price[data$price == ""] <- NA
sum(is.na(data$price))
data$price <- as.numeric(gsub("[\\$,]", "", data$price))
data$price_dist <- data$price
data$price_dist <- case_when(
data$price < 12000 ~ "< $12,000",
data$price >= 12000 & data$price < 22000 ~ "$12,000 - $21,999",
data$price >= 22000 & data$price < 32000 ~ "$20,000 - $31,999",
data$price >= 32000 & data$price < 50000 ~ "$32,000 - $49,999",
data$price >= 50000 & data$price < 80000 ~ "$50,000 - $79,999",
data$price >= 80000 ~ "> $80,000",
TRUE ~ "Unknown"
)
print(table(data$price_dist, useNA = "ifany"))
stargazer(data, type = "text", title = "Descriptive Statistics", digits = 3, out = "summary_table.txt")
View(data)
str(data)
head(data)
stargazer(data, type = "text", title = "Descriptive Statistics", digits = 3, out = "summary_table.txt")
str(data)
# Generate frequency tables for categorical variables
categorical_vars <- c("brand", "model_year", "fuel_type", "cylinder_arrangement", "transmission",
"ext_col", "int_col", "model_year_dist", "milage_dist", "price_dist")
freq_tables <- lapply(data[categorical_vars], table)
names(freq_tables) <- categorical_vars
# Print frequency tables
freq_tables
knitr::opts_chunk$set(echo = TRUE)
knitr::clean_cache()
library(tidyverse)
library(ggplot2)
library(dplyr)
library(gridExtra)
library(caret)
library(glmnet)
library(cluster)
library(kohonen)
library(stargazer)
library(reshape2)
library(cluster)
library(caret)
library(FNN)
library(gridExtra)
library(gbm)
library(caret)
library(randomForest)
library(nnet)
data <- read.csv("used_cars.csv")
data[data == ""] <- NA
data <- apply(data, 2, function(x) gsub("^\\s*$", NA, x))
num_missing <- sum(!complete.cases(data))
#num_missing
data <- as.data.frame(data)
data$clean_title[data$clean_title == ""] <- NA
data$brand[data$brand == ""] <- NA
brand_summary <- table(data$brand, useNA = "ifany")
print(brand_summary)
distinct_count <- length(unique(names(brand_summary)))
print(distinct_count)
data$brand <- case_when(
str_detect(data$brand, "(?i)Hyundai|Kia|Nissan|Toyota|Chevrolet|Ford|Honda|Volkswagen|Mazda|Buick|Subaru|Mitsubishi") ~ "Cheap",
str_detect(data$brand, "(?i)Jeep|GMC|Chrysler|Dodge|Ram|Acura|Volvo|MINI|Infiniti|Genesis|Lincoln") ~ "Mainstream",
str_detect(data$brand, "(?i)BMW|Mercedes-Benz|Audi|Lexus|Porsche|Jaguar|Cadillac|Land|Tesla|Maserati|Ferrari|Bentley|Rolls-Royce|Lamborghini|Aston|McLaren|Maybach|Lucid") ~ "Luxury",
TRUE ~ "Other"
)
brand_summary <- table(data$brand, useNA = "ifany")
print(brand_summary)
data$model_year[data$model_year == ""] <- NA
model_year_summary <- table(data$model_year, useNA = "ifany")
print(model_year_summary)
data$model_year_dist <- data$model_year
data$model_year_dist <- case_when(
data$model_year_dist >= 2020 ~ "New",
data$model_year_dist >= 2015 & data$model_year_dist < 2020 ~ "Recent",
data$model_year_dist >= 2010 & data$model_year_dist < 2015 ~ "Old",
data$model_year_dist < 2010 ~ "Very Old",
TRUE ~ "Unknown"
)
model_year_dist_summary <- table(data$model_year_dist, useNA = "ifany")
print(model_year_dist_summary)
data$milage[data$milage == ""] <- NA
milage_summary <- table(data$milage, useNA = "ifany")
#print(milage_summary)
data$milage <- as.numeric(gsub(",", "", gsub(" mi\\.", "", data$milage)))
data$milage_dist <- data$milage
data$milage_dist <- case_when(
data$milage_dist < 25000 ~ "Low Mileage",
data$milage_dist >= 25000 & data$milage_dist <= 75000 ~ "Moderate Mileage",
data$milage_dist > 75000 & data$milage_dist <= 150000 ~ "High Mileage",
data$milage_dist> 150000 ~ "Very High Mileage",
TRUE ~ "Unknown"
)
milage_dist_summary <- table(data$milage_dist, useNA = "ifany")
print(milage_dist_summary)
data <- as.data.frame(data)
data <- data %>%
mutate(horsepower = str_extract(engine, "\\d+\\.\\d+HP")) %>%
mutate(horsepower = as.numeric(str_replace(horsepower, "HP", "")))
data <- data %>%
mutate(engine_size = str_extract(engine, "\\d+\\.\\d+L")) %>%
mutate(engine_size = as.numeric(str_replace(engine_size, "L", "")))
data <- data %>%
mutate(cylinders = str_extract(engine, "\\d+ Cylinder")) %>%
mutate(cylinders = as.numeric(str_extract(cylinders, "\\d+")))
data <- data %>%
mutate(fuel_type = case_when(
str_detect(engine, "(?i)Gasoline") ~ "Gasoline",
str_detect(engine, "(?i)Hybrid") ~ "Hybrid",
str_detect(engine, "(?i)Electric") ~ "Electric",
str_detect(engine, "(?i)Diesel") ~ "Diesel",
TRUE ~ "Other"
))
data <- data %>%
mutate(cylinder_arrangement = case_when(
str_detect(engine, "(?i)V") ~ "V",
str_detect(engine, "(?i)Flat") ~ "Flat",
str_detect(engine, "(?i)Straight") ~ "Straight",
TRUE ~ "N"
))
cols <- colnames(data)
engine_index <- which(cols == "engine")
data <- data %>%
select(
all_of(cols[1:(engine_index-1)]),
horsepower, engine_size, cylinders,
cylinder_arrangement, fuel_type,
all_of(cols[(engine_index + 1):length(cols)][!(cols[(engine_index + 1):length(cols)] %in%
c("horsepower", "engine_size", "cylinders", "fuel_type", "cylinder_arrangement"))])
)
sum(is.na(data$horsepower))
data$horsepower[is.na(data$horsepower)] <- mean(data$horsepower, na.rm = TRUE)
data$engine_size[is.na(data$engine_size)] <- mean(data$engine_size, na.rm = TRUE)
data$cylinders[is.na(data$cylinders)] <- mean(data$cylinders, na.rm = TRUE)
data$transmission <- case_when(
str_detect(data$transmission, "(?i)A/T|Automatic|CVT") ~ "Automatic",
str_detect(data$transmission, "(?i)M/T|Manual") ~ "Manual",
TRUE ~ "Other"
)
transmission_summary <- table(data$transmission, useNA = "ifany")
print(transmission_summary)
data$ext_col[data$ext_col == ""] <- NA
sum(is.na(data$ext_col))
ext_col_summary <- table(data$ext_col, useNA = "ifany")
#print(ext_col_summary)
data$ext_col <- case_when(
str_detect(data$ext_col, "(?i)black|noir|onyx|obsidian|carbon|brown|bronze|espresso|dark") ~ "Dark",
str_detect(data$ext_col, "(?i)white|snow|pearl|alpine|glacier|frozen|chalk|beige|silver|grey|gray") ~ "Light",
str_detect(data$ext_col, "(?i)red|ruby|scarlet|garnet|firecracker|violet|flame|vermilion|pink|yellow|gold|orange|green|blue|blu|turquoise|aqua|lapis|denim|azure") ~ "Vivid",
is.na(data$ext_col) | data$ext_col == "" ~ "Unknown",
TRUE ~ "Other"
)
ext_col_summary <- table(data$ext_col, useNA = "ifany")
print(ext_col_summary)
data$ext_col[data$int_col == ""] <- NA
sum(is.na(data$int_col))
data$int_col <- case_when(
str_detect(data$int_col, "(?i)black|noir|onyx|obsidian|carbon|brown|bronze|espresso|dark") ~ "Dark",
str_detect(data$int_col, "(?i)white|snow|pearl|alpine|glacier|frozen|chalk|beige|silver|grey|gray") ~ "Light",
str_detect(data$int_col, "(?i)red|ruby|scarlet|garnet|firecracker|violet|flame|vermilion|pink|yellow|gold|orange|green|blue|turquoise|aqua|lapis|denim|azure") ~ "Vivid",
is.na(data$int_col) | data$int_col == "" | data$int_col == "-" ~ "Unknown",
TRUE ~ "Other"
)
int_col_summary <- table(data$int_col, useNA = "ifany")
print(int_col_summary)
data$ext_col[data$accident == ""] <- NA
sum(is.na(data$accident))
data$accident <- ifelse(data$accident == "None reported", 0,
ifelse(data$accident == "At least 1 accident or damage reported", 1, -1))
data$accident[is.na(data$accident)] <- 0
accident_summary <- table(data$accident, useNA = "ifany")
print(accident_summary)
clean_title_summary <- table(data$clean_title, useNA = "ifany")
print(clean_title_summary)
data$clean_title <- ifelse(data$clean_title == "Yes", 1, 0)
data$clean_title[is.na(data$clean_title)] <- 0
clean_title_summary <- table(data$clean_title, useNA = "ifany")
print(clean_title_summary)
data$price[data$price == ""] <- NA
sum(is.na(data$price))
data$price <- as.numeric(gsub("[\\$,]", "", data$price))
data$price_dist <- data$price
data$price_dist <- case_when(
data$price < 12000 ~ "< $12,000",
data$price >= 12000 & data$price < 22000 ~ "$12,000 - $21,999",
data$price >= 22000 & data$price < 32000 ~ "$20,000 - $31,999",
data$price >= 32000 & data$price < 50000 ~ "$32,000 - $49,999",
data$price >= 50000 & data$price < 80000 ~ "$50,000 - $79,999",
data$price >= 80000 ~ "> $80,000",
TRUE ~ "Unknown"
)
print(table(data$price_dist, useNA = "ifany"))
stargazer(data, type = "text", title = "Descriptive Statistics", digits = 3, out = "summary_table.txt")
#str(data)
# Generate frequency tables for categorical variables
categorical_vars <- c("brand", "model_year", "fuel_type", "cylinder_arrangement", "transmission",
"ext_col", "int_col", "model_year_dist", "milage_dist", "price_dist")
price_threshold <- quantile(data$price, 0.95, na.rm = TRUE)
filtered_data <- subset(data, price <= price_threshold)
ggplot(filtered_data, aes(x = price)) +
geom_histogram(binwidth = 5000, fill = "lightblue", color = "black", alpha = 0.7) +
labs(title = "Distribution of Price (Below 95th Percentile)",
x = "Price (USD)",
y = "Frequency") +
theme_minimal()
ggplot(data, aes(x = model_year)) +
geom_bar(fill = "lightblue", color = "black", alpha = 0.7) +
labs(title = "Distribution of Model Year",
x = "Model Year",
y = "Count") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels if needed
ggplot(data, aes(x = milage)) +
geom_histogram(binwidth = 10000, fill = "lightblue", color = "black", alpha = 0.7) +
labs(title = "Distribution of Mileage",
x = "Mileage (miles)",
y = "Frequency") +
theme_minimal()
# List of categorical variables to plot
categorical_vars <- c("brand", "fuel_type", "transmission", "ext_col",
"int_col", "model_year_dist", "milage_dist", "price_dist")
# Create individual plots for each categorical variable
plots <- list()
for (var in categorical_vars) {
p <- ggplot(data, aes_string(x = var)) +
geom_bar(fill = "lightblue", color = "black", alpha = 0.7) +
theme_minimal() +
labs(title = paste("Count Plot of", var), x = var, y = "Count") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
geom_text(stat='count', aes(label=..count..), vjust=-0.5)
plots[[var]] <- p
}
grid.arrange(grobs = plots[1:4], ncol = 2)
grid.arrange(grobs = plots[5:8], ncol = 2)
max_price <- 100000
# Creating individual boxplots with restricted y-axis range
p1 <- ggplot(data, aes(x = fuel_type, y = price)) +
geom_boxplot(fill = "skyblue") +
coord_cartesian(ylim = c(0, max_price)) + # Limit y-axis to zoom in
theme_minimal() +
labs(title = "Fuel Type vs. Price", x = "Fuel Type", y = "Price")
p2 <- ggplot(data, aes(x = cylinder_arrangement, y = price)) +
geom_boxplot(fill = "lightgreen") +
coord_cartesian(ylim = c(0, max_price)) + # Limit y-axis to zoom in
theme_minimal() +
labs(title = "Cylinder Arrangement vs. Price", x = "Cylinder Arrangement", y = "Price")
p3 <- ggplot(data, aes(x = transmission, y = price)) +
geom_boxplot(fill = "coral") +
coord_cartesian(ylim = c(0, max_price)) + # Limit y-axis to zoom in
theme_minimal() +
labs(title = "Transmission vs. Price", x = "Transmission", y = "Price")
p4 <- ggplot(data, aes(x = ext_col, y = price)) +
geom_boxplot(fill = "lightpink") +
coord_cartesian(ylim = c(0, max_price)) + # Limit y-axis to zoom in
theme_minimal() +
labs(title = "Exterior Color vs. Price", x = "Exterior Color", y = "Price")
p5 <- ggplot(data, aes(x = int_col, y = price)) +
geom_boxplot(fill = "lightsalmon") +
coord_cartesian(ylim = c(0, max_price)) + # Limit y-axis to zoom in
theme_minimal() +
labs(title = "Interior Color vs. Price", x = "Interior Color", y = "Price")
p6 <- ggplot(data, aes(x = brand, y = price)) +
geom_boxplot(fill = "lightskyblue") +
coord_cartesian(ylim = c(0, max_price)) + # Limit y-axis to zoom in
theme_minimal() +
labs(title = "Brand vs. Price", x = "Brand", y = "Price")
# Arrange plots into a grid
grid.arrange(p1, p2, p3, p4, p5, p6, ncol = 2)
# Select only numeric columns from the data
numeric_data <- data[sapply(data, is.numeric)]
# Calculate the correlation matrix using only the numeric columns
correlation_matrix <- cor(numeric_data, use = "complete.obs")
# Melt the correlation matrix for use in ggplot2
correlation_melted <- melt(correlation_matrix)
# Plot the heatmap using ggplot2 with correlation values labeled
ggplot(data = correlation_melted, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = "white") +
geom_text(aes(label = round(value, 2)), size = 4) +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1, 1), space = "Lab",
name = "Correlation") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, vjust = 1,
size = 10, hjust = 1)) +
coord_fixed() +
labs(title = "Correlation Heatmap")
features <- data[, c("milage", "horsepower", "engine_size",
"ext_col", "int_col", "cylinders",
"accident", "transmission", "fuel_type",
"cylinder_arrangement")]
# Identify numeric and categorical columns
numeric_cols <- sapply(features, is.numeric)
categorical_cols <- !numeric_cols
# Step 1: Scale numerical columns
features[, numeric_cols] <- scale(features[, numeric_cols])
# Step 2: One-hot encode categorical variables
# Convert categorical variables into dummy variables
categorical_dummies <- model.matrix(~ . - 1, data = features[, categorical_cols])
# Step 3: Combine scaled numerical and one-hot encoded categorical variables
combined_features <- cbind(features[, numeric_cols], categorical_dummies)
combined_features_matrix <- as.matrix(combined_features)
# Set up SOM grid
som_grid <- somgrid(xdim = 5, ydim = 5, topo = "hexagonal")  # Adjust grid size as needed
# Train the SOM
set.seed(123)
som_model <- som(X = combined_features_matrix, grid = som_grid, rlen = 100)  # Adjust 'rlen' for iterations
# Visualizations
# Training progress
plot(som_model, type = "changes", main = "Training Progress")
# Codebook vectors
plot(som_model, type = "codes", main = "Codebook Vectors")
# Mapping of samples onto the SOM
plot(som_model, type = "mapping", pchs = 19, main = "Sample Mapping")
# U-Matrix (distance between nodes)
plot(som_model, type = "dist.neighbours", main = "U-Matrix")
# Get cluster assignments from SOM model
som_clusters <- as.integer(som_model$unit.classif)
# Calculate silhouette scores
som_silhouette <- silhouette(som_clusters, dist(combined_features_matrix))
mean_som_silhouette <- mean(som_silhouette[, 3])
print(paste("Mean Silhouette Score for SOM:", mean_som_silhouette))
gower_dist <- daisy(combined_features, metric = "gower")
hclust_result <- hclust(as.dist(gower_dist), method = "ward.D2")
plot(hclust_result,
main = "Dendrogram for Hierarchical Clustering (Mixed Data)",
xlab = "Data Points",
ylab = "Height")
data$hclust_cluster <- cutree(hclust_result, k = 3)
print("Cluster sizes: ")
table(data$hclust_cluster)
hclust_silhouette <- silhouette(data$hclust_cluster, gower_dist)
mean_hclust_silhouette <- mean(hclust_silhouette[, 3])
print(paste("Mean Silhouette Score for Hierarchical Clustering:", mean_hclust_silhouette))
#Step 2: Perform PCA
pca_result <- prcomp(combined_features, center = TRUE, scale. = TRUE)
#Step 2: Analyze PCA variance to choose components
summary(pca_result)  # Check proportion of variance explained
explained_variance <- cumsum(pca_result$sdev^2) / sum(pca_result$sdev^2)
plot(explained_variance, type = "b",
xlab = "Number of Principal Components",
ylab = "Cumulative Explained Variance",
main = "Explained Variance by Principal Components")
#Select first 2 components for clustering
pca_data <- as.data.frame(pca_result$x[, 1:2])
#Step 3: Determine Optimal Number of Clusters for K-Means
#3.1 Silhouette Method
sil_scores <- numeric()
for (k in 2:10) {
set.seed(123)
kmeans_result <- stats::kmeans(pca_data, centers = k, nstart = 25)
sil_scores[k] <- mean(silhouette(kmeans_result$cluster, dist(pca_data))[, 3])
}
plot(2:10, sil_scores[2:10], type = "b", pch = 19,
xlab = "Number of Clusters",
ylab = "Average Silhouette Width",
main = "Silhouette Method for Optimal Clusters")
#Use the best number of clusters (e.g., from Silhouette Method or Elbow)
optimal_clusters <- which.max(sil_scores)
#Step 4: Apply K-means Clustering with Optimal Number of Clusters
set.seed(123)
kmeans_pca <- stats::kmeans(pca_data, centers = optimal_clusters, nstart = 25)
#Add cluster labels to original data
combined_features$pca_cluster <- as.factor(kmeans_pca$cluster)
#Visualize clustering in PCA space
plot(pca_data$PC1, pca_data$PC2,
col = kmeans_pca$cluster,
pch = 19,
xlab = "Principal Component 1",
ylab = "Principal Component 2",
main = paste("K-Means Clustering with", optimal_clusters, "Clusters"))
kmeans_silhouette <- silhouette(kmeans_pca$cluster, dist(pca_data))
mean_kmeans_silhouette <- mean(kmeans_silhouette[, 3])
print(paste("Mean Silhouette Score for K-Means Clustering:", mean_kmeans_silhouette))
data_for_model <- data.frame(
PC1 = pca_result$x[, 1],  # First principal component
PC2 = pca_result$x[, 2],  # Second principal component
cluster = as.numeric(combined_features$pca_cluster),  # Cluster labels as numeric
price = data$price  # Assuming `price` is already defined
)
# Step 1: Split data into training and testing sets
set.seed(123)
train_index <- createDataPartition(data_for_model$price, p = 0.5, list = FALSE)
train_data <- data_for_model[train_index, ]
test_data <- data_for_model[-train_index, ]
# Step 2: Elastic Net model training
# Create design matrices
x_train <- model.matrix(price ~ ., train_data)[, -1]  # Exclude intercept
y_train <- train_data$price
x_test <- model.matrix(price ~ ., test_data)[, -1]
y_test <- test_data$price
# Perform Elastic Net with cross-validation
set.seed(123)
cv_model <- cv.glmnet(x_train, y_train, alpha = 0.5, family = "gaussian")  # Alpha = 0.5 for Elastic Net
best_lambda <- cv_model$lambda.min  # Optimal lambda based on cross-validation
# Fit the final model
elastic_net_model <- glmnet(x_train, y_train, alpha = 0.5, lambda = best_lambda)
# Step 3: Predict and evaluate
predictions <- predict(elastic_net_model, newx = x_test)
# Calculate Normalized MSE
mean_price <- mean(y_test)
normalized_mse <- mean((predictions - y_test)^2) / mean_price^2
# Step 4.2: Bias-variance tradeoff plot (Normalized MSE vs Lambda)
normalized_mse_values <- cv_model$cvm / mean_price^2  # Normalize the cross-validation MSEs
plot(log(cv_model$lambda), normalized_mse_values,
type = "b",
xlab = "Log(Lambda)",
ylab = "Normalized Mean Squared Error (Normalized MSE)",
main = "Bias-Variance Tradeoff: Normalized MSE vs Lambda",
col = "blue", pch = 19)
abline(v = log(best_lambda), col = "red", lwd = 2, lty = 2)
#cat("Best lambda (minimizing cross-validation error):", best_lambda, "\n")
#cat("Alpha (Elastic Net mixing ratio):", 0.5, "\n")
# Step 1: Split the data into training and testing sets
set.seed(123)
train_index <- createDataPartition(data_for_model$price, p = 0.5, list = FALSE)
train_data <- data_for_model[train_index, ]
test_data <- data_for_model[-train_index, ]
# Step 2: Normalize the predictor variables (k-NN requires normalization)
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
train_data_normalized <- train_data
test_data_normalized <- test_data
train_data_normalized[, c("PC1", "PC2", "cluster")] <- lapply(
train_data_normalized[, c("PC1", "PC2", "cluster")],
normalize
)
test_data_normalized[, c("PC1", "PC2", "cluster")] <- lapply(
test_data_normalized[, c("PC1", "PC2", "cluster")],
normalize
)
# Step 3: Extract predictors and target variable
train_x <- train_data_normalized[, c("PC1", "PC2", "cluster")]
test_x <- test_data_normalized[, c("PC1", "PC2", "cluster")]
train_y <- train_data$price
test_y <- test_data$price
# Step 4: Tune k-NN model using cross-validation
set.seed(123)
tune_results <- train(
x = train_x,
y = train_y,
method = "knn",
trControl = trainControl(method = "cv", number = 5),  # 5-fold cross-validation
tuneGrid = data.frame(k = seq(3, 20, by = 2))
)
best_k <- tune_results$bestTune$k
# Step 5: Use the best k to predict on the test set
predictions <- knn(
train = train_x,
test = test_x,
cl = train_y,
k = best_k
)
# Step 6: Evaluate performance
predictions <- as.numeric(predictions)
mean_test_price <- mean(test_y)
normalized_mse <- mean((test_y - predictions)^2) / mean_test_price^2
cat("Best k:", best_k, "\n")
cat("Normalized MSE:", normalized_mse, "\n")
# Step 7: Visualize k-NN Regression with Different k-values
x_seq <- seq(min(train_data_normalized$PC1), max(train_data_normalized$PC1), length.out = 100)
knn_fit <- function(k, train_x, train_y, test_x) {
knn_pred <- knn.reg(train = as.matrix(train_x), test = as.matrix(test_x), y = train_y, k = k)
return(knn_pred$pred)
}
# Create multiple k values to visualize
k_values <- c(1, 5, 10, 19, 50)
plots <- list()
for (k in k_values) {
knn_predictions <- knn_fit(k, train_x = train_data_normalized$PC1, train_y = train_y, test_x = x_seq)
plot_data <- data.frame(
x_seq = x_seq,
knn_fit = knn_predictions
)
p <- ggplot() +
geom_point(data = train_data_normalized, aes(x = PC1, y = train_y), alpha = 0.5, size = 2) +
geom_line(data = plot_data, aes(x = x_seq, y = knn_fit), color = "orange", size = 1.2) +
labs(title = paste("k =", k, "k=19 optimal"), x = "PC1", y = "Price") +
theme_minimal()
plots[[paste0("k_", k)]] <- p
}
do.call(grid.arrange, c(plots, ncol = 3))
